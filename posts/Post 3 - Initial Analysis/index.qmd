---
title: "Final Project FB - working doc"
subtitle: "DACSS 695N Social Network Analysis"
author: "E. Song/ Felix Betancourt"
date: "April 21, 2024"
format: 
  html:
    toc: true
    toc-depth: 2
    toc-title: Contents
    toc-location: left
    code-fold: false
    html-math-method: katex
    theme: flatly
    smooth-scroll: true
    link-external-icon: true
    link-external-newwindow: true
    citations-hover: true
    footnotes-hover: true
    font-size: 80%
editor: visual
---

## Final Project - Network on subreddit r/politics

### Introduction

I am interested in understanding how social media users influence each other and create communities around specific topics.

Specifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.

### Research Question

In particular:

1.  How are Reddit users connected/related in the "Politics" subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?

2.  Are there different communities (networks) for Biden and Trump?

3.  Is there a relationship between "upvotes" for a post, number of comments and how it is related to the key users in the network?

```{r}
suppressWarnings({
suppressPackageStartupMessages(library(tidytext))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(quanteda))
suppressPackageStartupMessages(library(quanteda.textplots))
suppressPackageStartupMessages(library(janitor))
#library(RedditExtractoR)
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(data.table))
})

```

### Data

1.  I scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)

The code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.

```{r}
#politics_reddit <- find_thread_urls(subreddit = "politics", sort_by="new", period = "day")

#politics_reddit_comments <- get_thread_content(politics_reddit$url)

#Separate the lists into different objects

#politics_list1 <- politics_reddit_comments[[1]] 
#politics_list2 <- politics_reddit_comments[[2]]

### Saving all the lists in csv to avoid the need to scrap the data several times

#write.csv(politics_reddit, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv")

#write.csv(politics_list1, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv")

#write.csv(politics_list2, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv")

```

2.  This subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.

```{r}
getwd()
# Read large CSV file using fread
politik1 <- fread("politics_comments1.csv")
politik2 <- fread("politics_comments2.csv")
politik3 <- fread("politics_comments3.csv")

glimpse(politik1)
glimpse(politik2)
glimpse(politik3)

```

As we can see the the information in object "politik1" is redundant with the information in "politik2" so I won't use "politik1" at all. "Politik2" contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. "politik3" contain detailed comments on each post and the hierarchical sequence of comments to each post.

I'll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).

Let's do some data wrangling first:

```{r}

# Cleaning and wrangling

politik_df <- politik2 %>% select(-V1, -timestamp) #eliminating non-relevant columns
politik_df <- as_tibble(politik_df)
politik_df$date <- as.Date(politik_df$date, format = "%m/%d/%Y")


politik_df2 <- politik3 %>% select(-V1, -timestamp) #eliminating non-relevant columns
politik_df2 <- as_tibble(politik_df2)
politik_df2$date <- as.Date(politik_df2$date, format = "%m/%d/%Y")


```

Looking at the comments from user "Automoderator", it is like a Reddit moderator bot reminding rules of the forum, so I'll delete the rows belonging to AutoModerator". Also there are few commments where the author was "deleted".

```{r}

politik_df2 <- politik_df2[-(which(politik_df2$author %in% "AutoModerator")),]
politik_df3 <- politik_df2[-(which(politik_df2$author %in% "[deleted]")),]
length(unique(politik3$url))

```

Let's explore a bit about the authors.

```{r}

#let's create some tables to see frequencies and totals

#first I created a count column
politik_df3 <- politik_df3 %>% mutate(countid = "1")
politik_df3$countid <- as.numeric(politik_df3$countid)

#preparing tables
library(data.table)
politik_table2 <- data.table(politik_df3)

#total posts grouped by author
count_table2 <- politik_table2 %>% group_by(author) %>% summarise(Total_posts = sum(countid))
count_table2 <- count_table2 %>% arrange(desc(Total_posts))
print(count_table2)

summary_votes <- politik_table2 %>% group_by(author) %>% summarize(Total_Score = sum(score))
summary_votes <- summary_votes %>% arrange(desc(Total_Score))
print(summary_votes)

#Upvotes as a proportion of comments
summary_votes_ratio <- politik_table2 %>% group_by(author) %>% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))
summary_votes_ratio <- summary_votes_ratio %>% arrange(desc(Ratio_upvotes_per_comment))
print(summary_votes_ratio)

#How many authors (nodes) we have here?
length(unique(politik_df3$author))

```

In the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I'll select a sample of posts to analyze.

I'll select the top 1% posts with more comments.

```{r}
#first let's see the distribution of number of comments
percentiles <- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles)

```

Let's subset the df with the top 1% posts in terms of comments and let's see how many posts we have.

```{r}

subset_politik2 <- subset(politik_df, comments >= 1439 )
glimpse(subset_politik2)

length(unique(subset_politik2$author))

```

We got a df with 10 original posts and 10 authors, this is now a more "reasonable" data frame to analyze.

Now I need to identify these post into the "politik_df3" df which contain all the hierarchical comments network.

```{r}

subset_politik3 <- politik_df3 %>%
         filter(url %in% subset_politik2$url)

#let's see the df now 
glimpse(subset_politik3)

```

```{r}
#how many nodes (authors)?
length(unique(subset_politik3$author))
```

We got 982 posts but still +3.8k nodes, it sounds still high number of nodes.

I'll need a different approach.

I'll select 2 posts with a "median" number of comments. One post will be about Trump and another about Biden.

```{r}
suppressWarnings({
#First selecting posts with "Trump" or "Biden" included in the title of the post
#Filtering the titles that contain Trump
trump_df <- politik_df %>% filter(grepl("Trump", title))
trump_df$candidate <- "Trump"

#Let's check the distribution of number of comments
percentiles_trump <- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles_trump)

#Filtering the titles that contain Biden
biden_df <- politik_df %>% filter(grepl("Biden", title))
biden_df$candidate <- "Biden"

#Let's check the distribution of number of comments
percentiles_biden <- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles_biden)
})
```

```{r}

#Selecting the post for Trump and Biden that we will analyze
#Let's choose one post for each presidential candidate
#based on the median number of comments for each

#Trump
trump_post <- subset(trump_df, comments == 74 )

#Biden
biden_post <- subset(biden_df, comments == 67 )

```

Now I got the 2 main posts, let's explore a bit those 2 posts.

```{r}

#merging the previous df's
trump_biden_df <- rbind(trump_post, biden_post)
print(trump_biden_df$url)

#let's identify these posts in the politik3 df (containing all the details)
subset_politik3 <- politik_df3 %>%
         filter(url %in% trump_biden_df$url)

#creating a new column with the candidate related to the post
subset_politik3 <- subset_politik3 %>%
  mutate(candidate = case_when(
    url == "https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/" ~ "Trump",
    url == "https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/" ~ "Biden",
  ))

#let's see the df now 
glimpse(subset_politik3)

# Let's keep only the relevant columns
politik_final <- select(subset_politik3, c("url", "author", "score", "comment", "comment_id", "candidate"))

# Extracting the levels of each comment and its hierarchy
politik_final2 <- politik_final %>%
  mutate(Level = str_count(comment_id, pattern = "_") + 1,  # Count underscores to determine depth
         ParentID = ifelse(Level > 1, sapply(strsplit(comment_id, "_"), function(x) paste(x[-length(x)], collapse = "_")), NA))

length(unique(politik_final2$author))
length(unique(politik_final2$url))

```

Now we got 80 nodes (authors) from the 2 posts.

Now I am ready to work on this data set.

```{r}
#Rename level column as it represent more how deep/far is the comment
#from the initial post, we will use this later as an attribute
politik_final2 <- politik_final2 %>%
  rename(deep = Level)

#identify who is commenting on the same post
politik_final2 <- politik_final2 %>%
  mutate(level = substr(comment_id, 1, 2))

politik_final2$level <- str_replace_all(politik_final2$level, "_", "")

politik_final2 <- politik_final2 %>%
  mutate(level2 = substr(candidate, 1, 1))

politik_final2$comment_id2 <- paste(politik_final2$level2, politik_final2$level, sep = "_")

#Now I'll create a new object by keeping only the columns I need

politik_final3 <- select(politik_final2, c(-"comment_id", -"ParentID", -"level", -"level2"))


#Will create a attribute only object to use later
politik_attributes <- select(politik_final3, c("score", "candidate", "deep"))
head(politik_attributes)



```

I'll prepare the adjacency matrix.

```{r message=FALSE}

politik_m <- select(politik_final3, c("comment_id2", "author"))
glimpse(politik_m)

# Identify unique names and codes
unique_names <- unique(politik_final3$author)
unique_codes <- unique(politik_final3$comment_id2)

# Create an empty adjacency matrix
adj_matrix <- matrix(0, nrow = length(unique_names), ncol = length(unique_names),
                     dimnames = list(unique_names, unique_names))

#Populate the adjacency matrix based on shared codes
for (i in 1:length(unique_names)) {
  for (j in 1:length(unique_names)) {
    # Check if names i and j have the same code
    shared_code <- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],
                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])
    if (length(shared_code) > 0) {
      adj_matrix[unique_names[i], unique_names[j]] <- 1  # Set relationship to 1
    }
  }
}

# I'll eliminate loops in advance
diag(adj_matrix) <- 0


```

Now let's explore the Network

```{r message = FALSE}
suppressWarnings({
#load packages
library(network)
library(sna)
library(statnet)
})
politik.n <- network(adj_matrix)
politik.n

```

We got 80 nodes and 316 edges.

```{r}
#Dyads and Triads census
sna::dyad.census(politik.n)
sna::triad.census(politik.n)
sum(sna::triad.census(politik.n))

```

Seems that we have 82160 triads in total!

```{r}
#transitivity
gtrans(politik.n, mode="graph")

```

The transitivity coefficient is 0.85 which indicates a high level of cohesion.

Let's see the density

```{r}

# get network density: statnet
network::network.density(politik.n) #already exclude loops

```

This density of 0.05 indicates a relatively sparse network with few connections between nodes.

This combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.

Let's visualize the network

```{r}

# Plot the network
plot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = "Authors Network")

```

Without isolated nodes

```{r}

# Plot the network
plot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = "Authors Network")

```

Let's now include the Candidate as attribute

```{r}
#First I'll create a column with female-male true-false attribute
politik_attributes2 <- politik_attributes %>%
  mutate(
    biden = if_else(candidate == "Biden", "TRUE", "FALSE")
  )

#now let's see how females and males are interacting
nodeColors<-ifelse(politik_attributes2$biden,"dodgerblue","red")
plot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=T) #including isolated nodes
legend("bottomright", legend = c("Biden", "Trump"), col = c("dodgerblue", "red"), pch = c(21, 21), title = "Node Type")
```

```{r}
plot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=F) #excluding isolated nodes
legend("bottomright", legend = c("Biden", "Trump"), col = c("dodgerblue", "red"), pch = c(21, 21), title = "Node Type")
```
