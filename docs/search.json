[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Felix Betancourt’s Blog - DACSS - UMASS",
    "section": "",
    "text": "Final Project FB - Post 4 - Final post\n\n\nDACSS 695N Social Network Analysis\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nE. Song/ Felix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project FB - Post 3 - Initial Network Analysis\n\n\nDACSS 695N Social Network Analysis\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nE. Song/ Felix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nPost 2- Final Project FB - Scrapping Data and Exploratory Analysis - 695N Course\n\n\nDACSS 695N Social Network Analysis\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nE. Song/ Felix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Check-in #1 - 695N DACSS course - Spring 2024\n\n\n\n\n\n\nSocial Network\n\n\nFinal Project\n\n\nSocial Media\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nFelix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nTest post for 695N DACSS course - Spring 2024\n\n\n\n\n\n\nSocial Network\n\n\nBlog test\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nFelix Betancourt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Final Project Check-in 1/index.html",
    "href": "posts/Final Project Check-in 1/index.html",
    "title": "Final Project Check-in #1 - 695N DACSS course - Spring 2024",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\n\n\n\nI will scrap data from the Politics subreddit (r/politics) using R (RedditExtractoR package)\nThis subreddit has 8.5 million users, so the data can be very extensive, so I’ll scrap the post only for the last 3 months.\nI can extract the usernames, votes, and titles of the posts, among other information like the time stamp and a tree of comments for the original post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\n\n\n\n\n\nWith 8.5 million users I am not sure if I’ll be able to identify most frequent users (maybe the data can bevery fragmented), but I’ll try to identify users with certain number of posts or replies to narrow the network analysis."
  },
  {
    "objectID": "posts/Final Project Check-in 1/index.html#final-project-check-in-1",
    "href": "posts/Final Project Check-in 1/index.html#final-project-check-in-1",
    "title": "Final Project Check-in #1 - 695N DACSS course - Spring 2024",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\n\n\n\nI will scrap data from the Politics subreddit (r/politics) using R (RedditExtractoR package)\nThis subreddit has 8.5 million users, so the data can be very extensive, so I’ll scrap the post only for the last 3 months.\nI can extract the usernames, votes, and titles of the posts, among other information like the time stamp and a tree of comments for the original post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\n\n\n\n\n\nWith 8.5 million users I am not sure if I’ll be able to identify most frequent users (maybe the data can bevery fragmented), but I’ll try to identify users with certain number of posts or replies to narrow the network analysis."
  },
  {
    "objectID": "posts/My-new-post/index.html",
    "href": "posts/My-new-post/index.html",
    "title": "Test post for 695N DACSS course - Spring 2024",
    "section": "",
    "text": "This is a post with executable code.\n\neasy.sum &lt;- 1 + 1\n\nprint(easy.sum)\n\n[1] 2\n\n\n\neasy.div &lt;- 2/6\n\nprint(easy.div)\n\n[1] 0.3333333"
  },
  {
    "objectID": "posts/Post 2 - Scrapping the data and Exploratory Analysis/index.html",
    "href": "posts/Post 2 - Scrapping the data and Exploratory Analysis/index.html",
    "title": "Post 2- Final Project FB - Scrapping Data and Exploratory Analysis - 695N Course",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n\nWarning: package 'usethis' was built under R version 4.3.2\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nPackage version: 4.0.1\nUnicode version: 15.1\nICU version: 74.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n#library(RedditExtractoR)\nlibrary(RCurl)\n\nWarning: package 'RCurl' was built under R version 4.3.2\n\n\n\nAttaching package: 'RCurl'\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post only.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/695NBlog_Felix_Betancourt/posts/Post 2 - Scrapping the data and Exploratory Analysis\"\n\npolitik1 &lt;- read.csv(\"politics_comments1.csv\")\npolitik2 &lt;- read.csv(\"politics_comments2.csv\")\npolitik3 &lt;- read.csv(\"politics_comments3.csv\")\nhead(politik1)\n\n                                                                                                   X\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n    date_utc  timestamp\n1 2024-03-26 1711463500\n2 2024-03-26 1711463310\n3 2024-03-26 1711462666\n4 2024-03-26 1711462651\n5 2024-03-26 1711462570\n6 2024-03-26 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit comments\n1       politics       34\n2       politics       21\n3       politics      194\n4       politics       24\n5       politics       43\n6       politics       15\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n\ndim(politik1)\n\n[1] 983   8\n\nhead(politik2)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n             author      date  timestamp\n1  Cybertronian1512 3/26/2024 1711463500\n2           ban_hus 3/26/2024 1711463310\n3      coasterghost 3/26/2024 1711462666\n4           Quirkie 3/26/2024 1711462651\n5      texastribune 3/26/2024 1711462570\n6 thenationmagazine 3/26/2024 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit score upvotes downvotes up_ratio total_awards_received golds\n1       politics   304     304         0     0.95                     0     0\n2       politics   127     127         0     0.92                     0     0\n3       politics  1250    1250         0     0.95                     0     0\n4       politics   791     791         0     0.96                     0     0\n5       politics   421     421         0     0.91                     0     0\n6       politics   226     226         0     0.91                     0     0\n  cross_posts comments\n1           0       34\n2           0       21\n3           0      194\n4           0       24\n5           0       43\n6           0       15\n\ndim(politik2)\n\n[1] 983  16\n\nhead(politik3)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                          url\n1 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n3 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n4 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n5 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n6 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n         author      date  timestamp score upvotes downvotes golds\n1 AutoModerator 3/26/2024 1711463501     1       1         0     0\n2   EmmaLouLove 3/26/2024 1711465125    79      79         0     0\n3       ctguy54 3/26/2024 1711466287    41      41         0     0\n4   EmmaLouLove 3/26/2024 1711466455    13      13         0     0\n5        msfamf 3/26/2024 1711467091    19      19         0     0\n6   EmmaLouLove 3/26/2024 1711467635    13      13         0     0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        comment\n1 \\nAs a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)\\n\\nIn general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban. \\n\\nIf you see comments in violation of our rules, please report them.\\n\\n For those who have questions regarding any media outlets being posted on this subreddit, please click [here](https://www.reddit.com/r/politics/wiki/approveddomainslist) to review our details as to our approved domains list and outlet criteria.\\n \\n We are actively looking for new moderators.  If you have any interest in helping to make this subreddit a place for quality discussion, please fill out [this form](https://docs.google.com/forms/d/1y2swHD0KXFhStGFjW6k54r9iuMjzcFqDIVwuvdLBjSA).\\n \\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                US Solicitor General Elizabeth Prelogar reminded the Supreme Court the courts have no business questioning the FDA\\031s expertise on whether a drug should be approved or not.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This will not stop the conservatives on the court from believing that they know best. \\n\\nNext the court will decide whether you can take aspirin, Tylenol, or Motrin.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I am guessing even this SCOTUS knows that inserting themselves into FDA decisions will cause the floodgates to open.  What would be the next drug that would be brought before them?  It could be any drug advertised with the multiple warnings of side effects including death.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &gt;What would be the next drug that would be brought before them? \\n\\nIf current trends and Republican talking points are any indication it's birth control.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     True.  There is an ongoing attack by the conservative justices on Americans\\031 personal privacy rights.  \\n\\nClarence Thomas called for overturning the constitutional rights the court had affirmed for access to contraceptives and LGBTQ rights in his Roe opinion.  \\034In future cases, we should reconsider all of this Court\\031s substantive due process precedents, including Griswold, Lawrence, and Obergefell.\\035\\n\\nRepublicans seem hellbent on inserting themselves into anything they don\\031t agree with, whether it be food for needy children or a woman\\031s autonomy.  People should remember this when they vote.\n  comment_id\n1          1\n2          2\n3        2_1\n4      2_1_1\n5    2_1_1_1\n6  2_1_1_1_1\n\ndim(politik3)\n\n[1] 12414    11\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up-down votes, number of replies to the post. “politik3” contain detailed comments on each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\nhead(politik_df)\n\n# A tibble: 6 × 13\n  author       date       title text  subreddit score upvotes downvotes up_ratio\n  &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 Cybertronia… 2024-03-26 \"Sup… \"\"    politics    304     304         0     0.95\n2 ban_hus      2024-03-26 \"Why… \"\"    politics    127     127         0     0.92\n3 coasterghost 2024-03-26 \"201… \"\"    politics   1250    1250         0     0.95\n4 Quirkie      2024-03-26 \"Bid… \"\"    politics    791     791         0     0.96\n5 texastribune 2024-03-26 \"Fel… \"\"    politics    421     421         0     0.91\n6 thenationma… 2024-03-26 \"Tru… \"\"    politics    226     226         0     0.91\n# ℹ 4 more variables: total_awards_received &lt;int&gt;, golds &lt;int&gt;,\n#   cross_posts &lt;int&gt;, comments &lt;int&gt;\n\npolitik_df2 &lt;- politik3 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\nhead(politik_df2)\n\n# A tibble: 6 × 8\n  author        date       score upvotes downvotes golds comment      comment_id\n  &lt;chr&gt;         &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     \n1 AutoModerator 2024-03-26     1       1         0     0 \"\\nAs a rem… 1         \n2 EmmaLouLove   2024-03-26    79      79         0     0 \"US Solicit… 2         \n3 ctguy54       2024-03-26    41      41         0     0 \"This will … 2_1       \n4 EmmaLouLove   2024-03-26    13      13         0     0 \"I am guess… 2_1_1     \n5 msfamf        2024-03-26    19      19         0     0 \"&gt;What w… 2_1_1_1   \n6 EmmaLouLove   2024-03-26    13      13         0     0 \"True.  The… 2_1_1_1_1 \n\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nhead(politik_df3)\n\n# A tibble: 6 × 8\n  author      date       score upvotes downvotes golds comment        comment_id\n  &lt;chr&gt;       &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     \n1 EmmaLouLove 2024-03-26    79      79         0     0 \"US Solicitor… 2         \n2 ctguy54     2024-03-26    41      41         0     0 \"This will no… 2_1       \n3 EmmaLouLove 2024-03-26    13      13         0     0 \"I am guessin… 2_1_1     \n4 msfamf      2024-03-26    19      19         0     0 \"&gt;What wou… 2_1_1_1   \n5 EmmaLouLove 2024-03-26    13      13         0     0 \"True.  There… 2_1_1_1_1 \n6 msfamf      2024-03-26     7       7         0     0 \"I was readin… 2_1_1_1_1…\n\ndim(politik_df3)\n\n[1] 12063     8\n\n\nLet’s explore a bit the text in the comments with a word cloud before continuing with the Network analysis.\n\n#preparing for text analysis\npolitik_vec &lt;- as.vector(politik3$comment)\npolitik_vec2 &lt;- unlist(politik_vec, use.names = FALSE)\n\npolitik_corpus &lt;- corpus(politik_vec2)\n\npolitik_corpus_summary &lt;- summary(politik_corpus)\npolitik_tokens &lt;- tokens(politik_corpus)\n\npolitik_tokens2 &lt;- tokens(politik_corpus, remove_punct=TRUE, remove_numbers = T) %&gt;%\n              tokens_select(pattern=c(stopwords(\"en\"), \"s\", \"just\", \"get\", \"can\", \"like\", \"people\"),\n                            selection=\"remove\") %&gt;%\n             dfm()\n\n#word cloud\ntextplot_wordcloud(politik_tokens2, max.words = 200)\n\nWarning: max.words is deprecated; use max_words instead\n\n\n\n\n\n\n\n\n\nIt does seem that Trump is dminting the conversations.\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\nhead(politik_df3)\n\n# A tibble: 6 × 9\n  author     date       score upvotes downvotes golds comment comment_id countid\n  &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n1 EmmaLouLo… 2024-03-26    79      79         0     0 \"US So… 2                1\n2 ctguy54    2024-03-26    41      41         0     0 \"This … 2_1              1\n3 EmmaLouLo… 2024-03-26    13      13         0     0 \"I am … 2_1_1            1\n4 msfamf     2024-03-26    19      19         0     0 \"&gt;W… 2_1_1_1          1\n5 EmmaLouLo… 2024-03-26    13      13         0     0 \"True.… 2_1_1_1_1        1\n6 msfamf     2024-03-26     7       7         0     0 \"I was… 2_1_1_1_1…       1\n\n#preparing tables\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 6,520 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9          84\n 2 bignanoman                    40\n 3 Class_of_22                   34\n 4 Ferelwing                     33\n 5 LivingEnd44                   32\n 6 jimmydublets                  30\n 7 mymomknowsyourmom             25\n 8 grixorbatz                    23\n 9 sndcstle                      23\n10 Logical_Parameters            22\n# ℹ 6,510 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 6,520 × 2\n   author          Total_Score\n   &lt;chr&gt;                 &lt;int&gt;\n 1 BukkitCrab             8414\n 2 PalmettoAndMoon        6095\n 3 OokLeeNooma            5480\n 4 Poppa_Mo               4481\n 5 TheDudeBeto            3892\n 6 TintedApostle          3661\n 7 ImmoKnight             3343\n 8 Crazy-Nights           3032\n 9 HotPhilly              2809\n10 Jackinapox             2625\n# ℹ 6,510 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 6,520 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 OokLeeNooma                            5480 \n 2 TheDudeBeto                            3892 \n 3 Poppa_Mo                               2240.\n 4 ScotTheDuck                            2160 \n 5 AbandonedWaterPark                     2118 \n 6 rollingstone                           2096 \n 7 bocaciega                              1876 \n 8 jono9898                               1717 \n 9 PurpleCitron8                          1604 \n10 Jackinapox                             1312.\n# ℹ 6,510 more rows\n\n#Let's see how is the distribution of upvotes per comment\n#I may need to decide to analyze certain number of users only and the \n#level of interaction for those users could be a criteria de decide this\npercentiles &lt;- quantile(summary_votes_ratio$Ratio_upvotes_per_comment, probs = c(0.25, 0.50, 0.75, 0.90))\n\nprint(percentiles)\n\n      25%       50%       75%       90% \n 1.666667  3.666667 11.000000 31.161905 \n\n\nNow I am working on setting the data base for network analysis."
  },
  {
    "objectID": "posts/Post 2 - Scrapping the data and Exploratory Analysis/index.html#final-project---post-2---scrapping-and-exploring-the-data.",
    "href": "posts/Post 2 - Scrapping the data and Exploratory Analysis/index.html#final-project---post-2---scrapping-and-exploring-the-data.",
    "title": "Post 2- Final Project FB - Scrapping Data and Exploratory Analysis - 695N Course",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n\nWarning: package 'usethis' was built under R version 4.3.2\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nPackage version: 4.0.1\nUnicode version: 15.1\nICU version: 74.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n#library(RedditExtractoR)\nlibrary(RCurl)\n\nWarning: package 'RCurl' was built under R version 4.3.2\n\n\n\nAttaching package: 'RCurl'\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post only.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/695NBlog_Felix_Betancourt/posts/Post 2 - Scrapping the data and Exploratory Analysis\"\n\npolitik1 &lt;- read.csv(\"politics_comments1.csv\")\npolitik2 &lt;- read.csv(\"politics_comments2.csv\")\npolitik3 &lt;- read.csv(\"politics_comments3.csv\")\nhead(politik1)\n\n                                                                                                   X\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n    date_utc  timestamp\n1 2024-03-26 1711463500\n2 2024-03-26 1711463310\n3 2024-03-26 1711462666\n4 2024-03-26 1711462651\n5 2024-03-26 1711462570\n6 2024-03-26 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit comments\n1       politics       34\n2       politics       21\n3       politics      194\n4       politics       24\n5       politics       43\n6       politics       15\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n\ndim(politik1)\n\n[1] 983   8\n\nhead(politik2)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n             author      date  timestamp\n1  Cybertronian1512 3/26/2024 1711463500\n2           ban_hus 3/26/2024 1711463310\n3      coasterghost 3/26/2024 1711462666\n4           Quirkie 3/26/2024 1711462651\n5      texastribune 3/26/2024 1711462570\n6 thenationmagazine 3/26/2024 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit score upvotes downvotes up_ratio total_awards_received golds\n1       politics   304     304         0     0.95                     0     0\n2       politics   127     127         0     0.92                     0     0\n3       politics  1250    1250         0     0.95                     0     0\n4       politics   791     791         0     0.96                     0     0\n5       politics   421     421         0     0.91                     0     0\n6       politics   226     226         0     0.91                     0     0\n  cross_posts comments\n1           0       34\n2           0       21\n3           0      194\n4           0       24\n5           0       43\n6           0       15\n\ndim(politik2)\n\n[1] 983  16\n\nhead(politik3)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                          url\n1 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n3 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n4 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n5 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n6 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n         author      date  timestamp score upvotes downvotes golds\n1 AutoModerator 3/26/2024 1711463501     1       1         0     0\n2   EmmaLouLove 3/26/2024 1711465125    79      79         0     0\n3       ctguy54 3/26/2024 1711466287    41      41         0     0\n4   EmmaLouLove 3/26/2024 1711466455    13      13         0     0\n5        msfamf 3/26/2024 1711467091    19      19         0     0\n6   EmmaLouLove 3/26/2024 1711467635    13      13         0     0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        comment\n1 \\nAs a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)\\n\\nIn general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban. \\n\\nIf you see comments in violation of our rules, please report them.\\n\\n For those who have questions regarding any media outlets being posted on this subreddit, please click [here](https://www.reddit.com/r/politics/wiki/approveddomainslist) to review our details as to our approved domains list and outlet criteria.\\n \\n We are actively looking for new moderators.  If you have any interest in helping to make this subreddit a place for quality discussion, please fill out [this form](https://docs.google.com/forms/d/1y2swHD0KXFhStGFjW6k54r9iuMjzcFqDIVwuvdLBjSA).\\n \\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                US Solicitor General Elizabeth Prelogar reminded the Supreme Court the courts have no business questioning the FDA\\031s expertise on whether a drug should be approved or not.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This will not stop the conservatives on the court from believing that they know best. \\n\\nNext the court will decide whether you can take aspirin, Tylenol, or Motrin.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I am guessing even this SCOTUS knows that inserting themselves into FDA decisions will cause the floodgates to open.  What would be the next drug that would be brought before them?  It could be any drug advertised with the multiple warnings of side effects including death.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &gt;What would be the next drug that would be brought before them? \\n\\nIf current trends and Republican talking points are any indication it's birth control.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     True.  There is an ongoing attack by the conservative justices on Americans\\031 personal privacy rights.  \\n\\nClarence Thomas called for overturning the constitutional rights the court had affirmed for access to contraceptives and LGBTQ rights in his Roe opinion.  \\034In future cases, we should reconsider all of this Court\\031s substantive due process precedents, including Griswold, Lawrence, and Obergefell.\\035\\n\\nRepublicans seem hellbent on inserting themselves into anything they don\\031t agree with, whether it be food for needy children or a woman\\031s autonomy.  People should remember this when they vote.\n  comment_id\n1          1\n2          2\n3        2_1\n4      2_1_1\n5    2_1_1_1\n6  2_1_1_1_1\n\ndim(politik3)\n\n[1] 12414    11\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up-down votes, number of replies to the post. “politik3” contain detailed comments on each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\nhead(politik_df)\n\n# A tibble: 6 × 13\n  author       date       title text  subreddit score upvotes downvotes up_ratio\n  &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 Cybertronia… 2024-03-26 \"Sup… \"\"    politics    304     304         0     0.95\n2 ban_hus      2024-03-26 \"Why… \"\"    politics    127     127         0     0.92\n3 coasterghost 2024-03-26 \"201… \"\"    politics   1250    1250         0     0.95\n4 Quirkie      2024-03-26 \"Bid… \"\"    politics    791     791         0     0.96\n5 texastribune 2024-03-26 \"Fel… \"\"    politics    421     421         0     0.91\n6 thenationma… 2024-03-26 \"Tru… \"\"    politics    226     226         0     0.91\n# ℹ 4 more variables: total_awards_received &lt;int&gt;, golds &lt;int&gt;,\n#   cross_posts &lt;int&gt;, comments &lt;int&gt;\n\npolitik_df2 &lt;- politik3 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\nhead(politik_df2)\n\n# A tibble: 6 × 8\n  author        date       score upvotes downvotes golds comment      comment_id\n  &lt;chr&gt;         &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     \n1 AutoModerator 2024-03-26     1       1         0     0 \"\\nAs a rem… 1         \n2 EmmaLouLove   2024-03-26    79      79         0     0 \"US Solicit… 2         \n3 ctguy54       2024-03-26    41      41         0     0 \"This will … 2_1       \n4 EmmaLouLove   2024-03-26    13      13         0     0 \"I am guess… 2_1_1     \n5 msfamf        2024-03-26    19      19         0     0 \"&gt;What w… 2_1_1_1   \n6 EmmaLouLove   2024-03-26    13      13         0     0 \"True.  The… 2_1_1_1_1 \n\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nhead(politik_df3)\n\n# A tibble: 6 × 8\n  author      date       score upvotes downvotes golds comment        comment_id\n  &lt;chr&gt;       &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     \n1 EmmaLouLove 2024-03-26    79      79         0     0 \"US Solicitor… 2         \n2 ctguy54     2024-03-26    41      41         0     0 \"This will no… 2_1       \n3 EmmaLouLove 2024-03-26    13      13         0     0 \"I am guessin… 2_1_1     \n4 msfamf      2024-03-26    19      19         0     0 \"&gt;What wou… 2_1_1_1   \n5 EmmaLouLove 2024-03-26    13      13         0     0 \"True.  There… 2_1_1_1_1 \n6 msfamf      2024-03-26     7       7         0     0 \"I was readin… 2_1_1_1_1…\n\ndim(politik_df3)\n\n[1] 12063     8\n\n\nLet’s explore a bit the text in the comments with a word cloud before continuing with the Network analysis.\n\n#preparing for text analysis\npolitik_vec &lt;- as.vector(politik3$comment)\npolitik_vec2 &lt;- unlist(politik_vec, use.names = FALSE)\n\npolitik_corpus &lt;- corpus(politik_vec2)\n\npolitik_corpus_summary &lt;- summary(politik_corpus)\npolitik_tokens &lt;- tokens(politik_corpus)\n\npolitik_tokens2 &lt;- tokens(politik_corpus, remove_punct=TRUE, remove_numbers = T) %&gt;%\n              tokens_select(pattern=c(stopwords(\"en\"), \"s\", \"just\", \"get\", \"can\", \"like\", \"people\"),\n                            selection=\"remove\") %&gt;%\n             dfm()\n\n#word cloud\ntextplot_wordcloud(politik_tokens2, max.words = 200)\n\nWarning: max.words is deprecated; use max_words instead\n\n\n\n\n\n\n\n\n\nIt does seem that Trump is dminting the conversations.\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\nhead(politik_df3)\n\n# A tibble: 6 × 9\n  author     date       score upvotes downvotes golds comment comment_id countid\n  &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n1 EmmaLouLo… 2024-03-26    79      79         0     0 \"US So… 2                1\n2 ctguy54    2024-03-26    41      41         0     0 \"This … 2_1              1\n3 EmmaLouLo… 2024-03-26    13      13         0     0 \"I am … 2_1_1            1\n4 msfamf     2024-03-26    19      19         0     0 \"&gt;W… 2_1_1_1          1\n5 EmmaLouLo… 2024-03-26    13      13         0     0 \"True.… 2_1_1_1_1        1\n6 msfamf     2024-03-26     7       7         0     0 \"I was… 2_1_1_1_1…       1\n\n#preparing tables\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 6,520 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9          84\n 2 bignanoman                    40\n 3 Class_of_22                   34\n 4 Ferelwing                     33\n 5 LivingEnd44                   32\n 6 jimmydublets                  30\n 7 mymomknowsyourmom             25\n 8 grixorbatz                    23\n 9 sndcstle                      23\n10 Logical_Parameters            22\n# ℹ 6,510 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 6,520 × 2\n   author          Total_Score\n   &lt;chr&gt;                 &lt;int&gt;\n 1 BukkitCrab             8414\n 2 PalmettoAndMoon        6095\n 3 OokLeeNooma            5480\n 4 Poppa_Mo               4481\n 5 TheDudeBeto            3892\n 6 TintedApostle          3661\n 7 ImmoKnight             3343\n 8 Crazy-Nights           3032\n 9 HotPhilly              2809\n10 Jackinapox             2625\n# ℹ 6,510 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 6,520 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 OokLeeNooma                            5480 \n 2 TheDudeBeto                            3892 \n 3 Poppa_Mo                               2240.\n 4 ScotTheDuck                            2160 \n 5 AbandonedWaterPark                     2118 \n 6 rollingstone                           2096 \n 7 bocaciega                              1876 \n 8 jono9898                               1717 \n 9 PurpleCitron8                          1604 \n10 Jackinapox                             1312.\n# ℹ 6,510 more rows\n\n#Let's see how is the distribution of upvotes per comment\n#I may need to decide to analyze certain number of users only and the \n#level of interaction for those users could be a criteria de decide this\npercentiles &lt;- quantile(summary_votes_ratio$Ratio_upvotes_per_comment, probs = c(0.25, 0.50, 0.75, 0.90))\n\nprint(percentiles)\n\n      25%       50%       75%       90% \n 1.666667  3.666667 11.000000 31.161905 \n\n\nNow I am working on setting the data base for network analysis."
  },
  {
    "objectID": "posts/Post 2 - Scrapping the data and Exploratory Analysis/Final Project Post 2.html",
    "href": "posts/Post 2 - Scrapping the data and Exploratory Analysis/Final Project Post 2.html",
    "title": "Final Project FB - Scrapping Data and Exploratory Analysis",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n\nWarning: package 'usethis' was built under R version 4.3.2\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nPackage version: 4.0.1\nUnicode version: 15.1\nICU version: 74.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n#library(RedditExtractoR)\nlibrary(RCurl)\n\nWarning: package 'RCurl' was built under R version 4.3.2\n\n\n\nAttaching package: 'RCurl'\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post only.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/695NBlog_Felix_Betancourt/posts/Post 2 - Scrapping the data and Exploratory Analysis\"\n\npolitik1 &lt;- read.csv(\"politics_comments1.csv\")\npolitik2 &lt;- read.csv(\"politics_comments2.csv\")\npolitik3 &lt;- read.csv(\"politics_comments3.csv\")\nhead(politik1)\n\n                                                                                                   X\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n    date_utc  timestamp\n1 2024-03-26 1711463500\n2 2024-03-26 1711463310\n3 2024-03-26 1711462666\n4 2024-03-26 1711462651\n5 2024-03-26 1711462570\n6 2024-03-26 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit comments\n1       politics       34\n2       politics       21\n3       politics      194\n4       politics       24\n5       politics       43\n6       politics       15\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n\ndim(politik1)\n\n[1] 983   8\n\nhead(politik2)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n             author      date  timestamp\n1  Cybertronian1512 3/26/2024 1711463500\n2           ban_hus 3/26/2024 1711463310\n3      coasterghost 3/26/2024 1711462666\n4           Quirkie 3/26/2024 1711462651\n5      texastribune 3/26/2024 1711462570\n6 thenationmagazine 3/26/2024 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit score upvotes downvotes up_ratio total_awards_received golds\n1       politics   304     304         0     0.95                     0     0\n2       politics   127     127         0     0.92                     0     0\n3       politics  1250    1250         0     0.95                     0     0\n4       politics   791     791         0     0.96                     0     0\n5       politics   421     421         0     0.91                     0     0\n6       politics   226     226         0     0.91                     0     0\n  cross_posts comments\n1           0       34\n2           0       21\n3           0      194\n4           0       24\n5           0       43\n6           0       15\n\ndim(politik2)\n\n[1] 983  16\n\nhead(politik3)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                          url\n1 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n3 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n4 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n5 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n6 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n         author      date  timestamp score upvotes downvotes golds\n1 AutoModerator 3/26/2024 1711463501     1       1         0     0\n2   EmmaLouLove 3/26/2024 1711465125    79      79         0     0\n3       ctguy54 3/26/2024 1711466287    41      41         0     0\n4   EmmaLouLove 3/26/2024 1711466455    13      13         0     0\n5        msfamf 3/26/2024 1711467091    19      19         0     0\n6   EmmaLouLove 3/26/2024 1711467635    13      13         0     0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        comment\n1 \\nAs a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)\\n\\nIn general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban. \\n\\nIf you see comments in violation of our rules, please report them.\\n\\n For those who have questions regarding any media outlets being posted on this subreddit, please click [here](https://www.reddit.com/r/politics/wiki/approveddomainslist) to review our details as to our approved domains list and outlet criteria.\\n \\n We are actively looking for new moderators.  If you have any interest in helping to make this subreddit a place for quality discussion, please fill out [this form](https://docs.google.com/forms/d/1y2swHD0KXFhStGFjW6k54r9iuMjzcFqDIVwuvdLBjSA).\\n \\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                US Solicitor General Elizabeth Prelogar reminded the Supreme Court the courts have no business questioning the FDA\\031s expertise on whether a drug should be approved or not.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This will not stop the conservatives on the court from believing that they know best. \\n\\nNext the court will decide whether you can take aspirin, Tylenol, or Motrin.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I am guessing even this SCOTUS knows that inserting themselves into FDA decisions will cause the floodgates to open.  What would be the next drug that would be brought before them?  It could be any drug advertised with the multiple warnings of side effects including death.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &gt;What would be the next drug that would be brought before them? \\n\\nIf current trends and Republican talking points are any indication it's birth control.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     True.  There is an ongoing attack by the conservative justices on Americans\\031 personal privacy rights.  \\n\\nClarence Thomas called for overturning the constitutional rights the court had affirmed for access to contraceptives and LGBTQ rights in his Roe opinion.  \\034In future cases, we should reconsider all of this Court\\031s substantive due process precedents, including Griswold, Lawrence, and Obergefell.\\035\\n\\nRepublicans seem hellbent on inserting themselves into anything they don\\031t agree with, whether it be food for needy children or a woman\\031s autonomy.  People should remember this when they vote.\n  comment_id\n1          1\n2          2\n3        2_1\n4      2_1_1\n5    2_1_1_1\n6  2_1_1_1_1\n\ndim(politik3)\n\n[1] 12414    11\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up-down votes, number of replies to the post. “politik3” contain detailed comments on each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\nhead(politik_df)\n\n# A tibble: 6 × 13\n  author       date       title text  subreddit score upvotes downvotes up_ratio\n  &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 Cybertronia… 2024-03-26 \"Sup… \"\"    politics    304     304         0     0.95\n2 ban_hus      2024-03-26 \"Why… \"\"    politics    127     127         0     0.92\n3 coasterghost 2024-03-26 \"201… \"\"    politics   1250    1250         0     0.95\n4 Quirkie      2024-03-26 \"Bid… \"\"    politics    791     791         0     0.96\n5 texastribune 2024-03-26 \"Fel… \"\"    politics    421     421         0     0.91\n6 thenationma… 2024-03-26 \"Tru… \"\"    politics    226     226         0     0.91\n# ℹ 4 more variables: total_awards_received &lt;int&gt;, golds &lt;int&gt;,\n#   cross_posts &lt;int&gt;, comments &lt;int&gt;\n\npolitik_df2 &lt;- politik3 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\nhead(politik_df2)\n\n# A tibble: 6 × 8\n  author        date       score upvotes downvotes golds comment      comment_id\n  &lt;chr&gt;         &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     \n1 AutoModerator 2024-03-26     1       1         0     0 \"\\nAs a rem… 1         \n2 EmmaLouLove   2024-03-26    79      79         0     0 \"US Solicit… 2         \n3 ctguy54       2024-03-26    41      41         0     0 \"This will … 2_1       \n4 EmmaLouLove   2024-03-26    13      13         0     0 \"I am guess… 2_1_1     \n5 msfamf        2024-03-26    19      19         0     0 \"&gt;What w… 2_1_1_1   \n6 EmmaLouLove   2024-03-26    13      13         0     0 \"True.  The… 2_1_1_1_1 \n\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nhead(politik_df3)\n\n# A tibble: 6 × 8\n  author      date       score upvotes downvotes golds comment        comment_id\n  &lt;chr&gt;       &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     \n1 EmmaLouLove 2024-03-26    79      79         0     0 \"US Solicitor… 2         \n2 ctguy54     2024-03-26    41      41         0     0 \"This will no… 2_1       \n3 EmmaLouLove 2024-03-26    13      13         0     0 \"I am guessin… 2_1_1     \n4 msfamf      2024-03-26    19      19         0     0 \"&gt;What wou… 2_1_1_1   \n5 EmmaLouLove 2024-03-26    13      13         0     0 \"True.  There… 2_1_1_1_1 \n6 msfamf      2024-03-26     7       7         0     0 \"I was readin… 2_1_1_1_1…\n\ndim(politik_df3)\n\n[1] 12063     8\n\n\nLet’s explore a bit the text in the comments with a word cloud before continuing with the Network analysis.\n\n#preparing for text analysis\npolitik_vec &lt;- as.vector(politik3$comment)\npolitik_vec2 &lt;- unlist(politik_vec, use.names = FALSE)\n\npolitik_corpus &lt;- corpus(politik_vec2)\n\npolitik_corpus_summary &lt;- summary(politik_corpus)\npolitik_tokens &lt;- tokens(politik_corpus)\n\npolitik_tokens2 &lt;- tokens(politik_corpus, remove_punct=TRUE, remove_numbers = T) %&gt;%\n              tokens_select(pattern=c(stopwords(\"en\"), \"s\", \"just\", \"get\", \"can\", \"like\", \"people\"),\n                            selection=\"remove\") %&gt;%\n             dfm()\n\n#word cloud\ntextplot_wordcloud(politik_tokens2, max.words = 200)\n\nWarning: max.words is deprecated; use max_words instead\n\n\n\n\n\nIt does seem that Trump is dminting the conversations.\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\nhead(politik_df3)\n\n# A tibble: 6 × 9\n  author     date       score upvotes downvotes golds comment comment_id countid\n  &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n1 EmmaLouLo… 2024-03-26    79      79         0     0 \"US So… 2                1\n2 ctguy54    2024-03-26    41      41         0     0 \"This … 2_1              1\n3 EmmaLouLo… 2024-03-26    13      13         0     0 \"I am … 2_1_1            1\n4 msfamf     2024-03-26    19      19         0     0 \"&gt;W… 2_1_1_1          1\n5 EmmaLouLo… 2024-03-26    13      13         0     0 \"True.… 2_1_1_1_1        1\n6 msfamf     2024-03-26     7       7         0     0 \"I was… 2_1_1_1_1…       1\n\n#preparing tables\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 6,520 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9          84\n 2 bignanoman                    40\n 3 Class_of_22                   34\n 4 Ferelwing                     33\n 5 LivingEnd44                   32\n 6 jimmydublets                  30\n 7 mymomknowsyourmom             25\n 8 grixorbatz                    23\n 9 sndcstle                      23\n10 Logical_Parameters            22\n# ℹ 6,510 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 6,520 × 2\n   author          Total_Score\n   &lt;chr&gt;                 &lt;int&gt;\n 1 BukkitCrab             8414\n 2 PalmettoAndMoon        6095\n 3 OokLeeNooma            5480\n 4 Poppa_Mo               4481\n 5 TheDudeBeto            3892\n 6 TintedApostle          3661\n 7 ImmoKnight             3343\n 8 Crazy-Nights           3032\n 9 HotPhilly              2809\n10 Jackinapox             2625\n# ℹ 6,510 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 6,520 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 OokLeeNooma                            5480 \n 2 TheDudeBeto                            3892 \n 3 Poppa_Mo                               2240.\n 4 ScotTheDuck                            2160 \n 5 AbandonedWaterPark                     2118 \n 6 rollingstone                           2096 \n 7 bocaciega                              1876 \n 8 jono9898                               1717 \n 9 PurpleCitron8                          1604 \n10 Jackinapox                             1312.\n# ℹ 6,510 more rows\n\n#Let's see how is the distribution of upvotes per comment\n#I may need to decide to analyze certain number of users only and the \n#level of interaction for those users could be a criteria de decide this\npercentiles &lt;- quantile(summary_votes_ratio$Ratio_upvotes_per_comment, probs = c(0.25, 0.50, 0.75, 0.90))\n\nprint(percentiles)\n\n      25%       50%       75%       90% \n 1.666667  3.666667 11.000000 31.161905 \n\n\nNow I am working on setting the data base for network analysis."
  },
  {
    "objectID": "posts/Post 2 - Scrapping the data and Exploratory Analysis/Final Project Post 2.html#final-project---post-2---scrapping-and-exploring-the-data.",
    "href": "posts/Post 2 - Scrapping the data and Exploratory Analysis/Final Project Post 2.html#final-project---post-2---scrapping-and-exploring-the-data.",
    "title": "Final Project FB - Scrapping Data and Exploratory Analysis",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n\nWarning: package 'usethis' was built under R version 4.3.2\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nPackage version: 4.0.1\nUnicode version: 15.1\nICU version: 74.1\nParallel computing: 8 of 8 threads used.\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n#library(RedditExtractoR)\nlibrary(RCurl)\n\nWarning: package 'RCurl' was built under R version 4.3.2\n\n\n\nAttaching package: 'RCurl'\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post only.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/695NBlog_Felix_Betancourt/posts/Post 2 - Scrapping the data and Exploratory Analysis\"\n\npolitik1 &lt;- read.csv(\"politics_comments1.csv\")\npolitik2 &lt;- read.csv(\"politics_comments2.csv\")\npolitik3 &lt;- read.csv(\"politics_comments3.csv\")\nhead(politik1)\n\n                                                                                                   X\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n    date_utc  timestamp\n1 2024-03-26 1711463500\n2 2024-03-26 1711463310\n3 2024-03-26 1711462666\n4 2024-03-26 1711462651\n5 2024-03-26 1711462570\n6 2024-03-26 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit comments\n1       politics       34\n2       politics       21\n3       politics      194\n4       politics       24\n5       politics       43\n6       politics       15\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n\ndim(politik1)\n\n[1] 983   8\n\nhead(politik2)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                                   url\n1          https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2   https://www.reddit.com/r/politics/comments/1bo99ng/why_the_supreme_court_abortion_pill_case_is_so/\n3   https://www.reddit.com/r/politics/comments/1bo90vd/2015_securities_fraud_charges_against_texas_ag/\n4    https://www.reddit.com/r/politics/comments/1bo90oj/biden_campaign_calls_trump_weak_and_desperate/\n5 https://www.reddit.com/r/politics/comments/1bo8zhu/felony_securities_fraud_charges_against_attorney/\n6   https://www.reddit.com/r/politics/comments/1bo8u1n/trump_is_up_to_his_old_tricks_to_pay_his_bills/\n             author      date  timestamp\n1  Cybertronian1512 3/26/2024 1711463500\n2           ban_hus 3/26/2024 1711463310\n3      coasterghost 3/26/2024 1711462666\n4           Quirkie 3/26/2024 1711462651\n5      texastribune 3/26/2024 1711462570\n6 thenationmagazine 3/26/2024 1711462189\n                                                                                               title\n1                Supreme Court starts arguments as Biden administration defends abortion pill access\n2                               Why the Supreme Court abortion pill case is so fraught for the right\n3   2015 securities fraud charges against Texas AG Ken Paxton to be dropped in deal with prosecutors\n4                 Biden campaign calls Trump \\030weak and desperate\\031 after New York court hearing\n5 Felony securities fraud charges against Attorney General Ken Paxton to be dropped after nine years\n6                                                     Trump Is Up to His Old Tricks to Pay His Bills\n  text subreddit score upvotes downvotes up_ratio total_awards_received golds\n1       politics   304     304         0     0.95                     0     0\n2       politics   127     127         0     0.92                     0     0\n3       politics  1250    1250         0     0.95                     0     0\n4       politics   791     791         0     0.96                     0     0\n5       politics   421     421         0     0.91                     0     0\n6       politics   226     226         0     0.91                     0     0\n  cross_posts comments\n1           0       34\n2           0       21\n3           0      194\n4           0       24\n5           0       43\n6           0       15\n\ndim(politik2)\n\n[1] 983  16\n\nhead(politik3)\n\n  X\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n                                                                                          url\n1 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n2 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n3 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n4 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n5 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n6 https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_court_starts_arguments_as_biden/\n         author      date  timestamp score upvotes downvotes golds\n1 AutoModerator 3/26/2024 1711463501     1       1         0     0\n2   EmmaLouLove 3/26/2024 1711465125    79      79         0     0\n3       ctguy54 3/26/2024 1711466287    41      41         0     0\n4   EmmaLouLove 3/26/2024 1711466455    13      13         0     0\n5        msfamf 3/26/2024 1711467091    19      19         0     0\n6   EmmaLouLove 3/26/2024 1711467635    13      13         0     0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        comment\n1 \\nAs a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)\\n\\nIn general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban. \\n\\nIf you see comments in violation of our rules, please report them.\\n\\n For those who have questions regarding any media outlets being posted on this subreddit, please click [here](https://www.reddit.com/r/politics/wiki/approveddomainslist) to review our details as to our approved domains list and outlet criteria.\\n \\n We are actively looking for new moderators.  If you have any interest in helping to make this subreddit a place for quality discussion, please fill out [this form](https://docs.google.com/forms/d/1y2swHD0KXFhStGFjW6k54r9iuMjzcFqDIVwuvdLBjSA).\\n \\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                US Solicitor General Elizabeth Prelogar reminded the Supreme Court the courts have no business questioning the FDA\\031s expertise on whether a drug should be approved or not.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This will not stop the conservatives on the court from believing that they know best. \\n\\nNext the court will decide whether you can take aspirin, Tylenol, or Motrin.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I am guessing even this SCOTUS knows that inserting themselves into FDA decisions will cause the floodgates to open.  What would be the next drug that would be brought before them?  It could be any drug advertised with the multiple warnings of side effects including death.\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &gt;What would be the next drug that would be brought before them? \\n\\nIf current trends and Republican talking points are any indication it's birth control.\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     True.  There is an ongoing attack by the conservative justices on Americans\\031 personal privacy rights.  \\n\\nClarence Thomas called for overturning the constitutional rights the court had affirmed for access to contraceptives and LGBTQ rights in his Roe opinion.  \\034In future cases, we should reconsider all of this Court\\031s substantive due process precedents, including Griswold, Lawrence, and Obergefell.\\035\\n\\nRepublicans seem hellbent on inserting themselves into anything they don\\031t agree with, whether it be food for needy children or a woman\\031s autonomy.  People should remember this when they vote.\n  comment_id\n1          1\n2          2\n3        2_1\n4      2_1_1\n5    2_1_1_1\n6  2_1_1_1_1\n\ndim(politik3)\n\n[1] 12414    11\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up-down votes, number of replies to the post. “politik3” contain detailed comments on each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\nhead(politik_df)\n\n# A tibble: 6 × 13\n  author       date       title text  subreddit score upvotes downvotes up_ratio\n  &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 Cybertronia… 2024-03-26 \"Sup… \"\"    politics    304     304         0     0.95\n2 ban_hus      2024-03-26 \"Why… \"\"    politics    127     127         0     0.92\n3 coasterghost 2024-03-26 \"201… \"\"    politics   1250    1250         0     0.95\n4 Quirkie      2024-03-26 \"Bid… \"\"    politics    791     791         0     0.96\n5 texastribune 2024-03-26 \"Fel… \"\"    politics    421     421         0     0.91\n6 thenationma… 2024-03-26 \"Tru… \"\"    politics    226     226         0     0.91\n# ℹ 4 more variables: total_awards_received &lt;int&gt;, golds &lt;int&gt;,\n#   cross_posts &lt;int&gt;, comments &lt;int&gt;\n\npolitik_df2 &lt;- politik3 %&gt;% select(-url, -X, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\nhead(politik_df2)\n\n# A tibble: 6 × 8\n  author        date       score upvotes downvotes golds comment      comment_id\n  &lt;chr&gt;         &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     \n1 AutoModerator 2024-03-26     1       1         0     0 \"\\nAs a rem… 1         \n2 EmmaLouLove   2024-03-26    79      79         0     0 \"US Solicit… 2         \n3 ctguy54       2024-03-26    41      41         0     0 \"This will … 2_1       \n4 EmmaLouLove   2024-03-26    13      13         0     0 \"I am guess… 2_1_1     \n5 msfamf        2024-03-26    19      19         0     0 \"&gt;What w… 2_1_1_1   \n6 EmmaLouLove   2024-03-26    13      13         0     0 \"True.  The… 2_1_1_1_1 \n\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nhead(politik_df3)\n\n# A tibble: 6 × 8\n  author      date       score upvotes downvotes golds comment        comment_id\n  &lt;chr&gt;       &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     \n1 EmmaLouLove 2024-03-26    79      79         0     0 \"US Solicitor… 2         \n2 ctguy54     2024-03-26    41      41         0     0 \"This will no… 2_1       \n3 EmmaLouLove 2024-03-26    13      13         0     0 \"I am guessin… 2_1_1     \n4 msfamf      2024-03-26    19      19         0     0 \"&gt;What wou… 2_1_1_1   \n5 EmmaLouLove 2024-03-26    13      13         0     0 \"True.  There… 2_1_1_1_1 \n6 msfamf      2024-03-26     7       7         0     0 \"I was readin… 2_1_1_1_1…\n\ndim(politik_df3)\n\n[1] 12063     8\n\n\nLet’s explore a bit the text in the comments with a word cloud before continuing with the Network analysis.\n\n#preparing for text analysis\npolitik_vec &lt;- as.vector(politik3$comment)\npolitik_vec2 &lt;- unlist(politik_vec, use.names = FALSE)\n\npolitik_corpus &lt;- corpus(politik_vec2)\n\npolitik_corpus_summary &lt;- summary(politik_corpus)\npolitik_tokens &lt;- tokens(politik_corpus)\n\npolitik_tokens2 &lt;- tokens(politik_corpus, remove_punct=TRUE, remove_numbers = T) %&gt;%\n              tokens_select(pattern=c(stopwords(\"en\"), \"s\", \"just\", \"get\", \"can\", \"like\", \"people\"),\n                            selection=\"remove\") %&gt;%\n             dfm()\n\n#word cloud\ntextplot_wordcloud(politik_tokens2, max.words = 200)\n\nWarning: max.words is deprecated; use max_words instead\n\n\n\n\n\nIt does seem that Trump is dminting the conversations.\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\nhead(politik_df3)\n\n# A tibble: 6 × 9\n  author     date       score upvotes downvotes golds comment comment_id countid\n  &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;   &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n1 EmmaLouLo… 2024-03-26    79      79         0     0 \"US So… 2                1\n2 ctguy54    2024-03-26    41      41         0     0 \"This … 2_1              1\n3 EmmaLouLo… 2024-03-26    13      13         0     0 \"I am … 2_1_1            1\n4 msfamf     2024-03-26    19      19         0     0 \"&gt;W… 2_1_1_1          1\n5 EmmaLouLo… 2024-03-26    13      13         0     0 \"True.… 2_1_1_1_1        1\n6 msfamf     2024-03-26     7       7         0     0 \"I was… 2_1_1_1_1…       1\n\n#preparing tables\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 6,520 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9          84\n 2 bignanoman                    40\n 3 Class_of_22                   34\n 4 Ferelwing                     33\n 5 LivingEnd44                   32\n 6 jimmydublets                  30\n 7 mymomknowsyourmom             25\n 8 grixorbatz                    23\n 9 sndcstle                      23\n10 Logical_Parameters            22\n# ℹ 6,510 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 6,520 × 2\n   author          Total_Score\n   &lt;chr&gt;                 &lt;int&gt;\n 1 BukkitCrab             8414\n 2 PalmettoAndMoon        6095\n 3 OokLeeNooma            5480\n 4 Poppa_Mo               4481\n 5 TheDudeBeto            3892\n 6 TintedApostle          3661\n 7 ImmoKnight             3343\n 8 Crazy-Nights           3032\n 9 HotPhilly              2809\n10 Jackinapox             2625\n# ℹ 6,510 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 6,520 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 OokLeeNooma                            5480 \n 2 TheDudeBeto                            3892 \n 3 Poppa_Mo                               2240.\n 4 ScotTheDuck                            2160 \n 5 AbandonedWaterPark                     2118 \n 6 rollingstone                           2096 \n 7 bocaciega                              1876 \n 8 jono9898                               1717 \n 9 PurpleCitron8                          1604 \n10 Jackinapox                             1312.\n# ℹ 6,510 more rows\n\n#Let's see how is the distribution of upvotes per comment\n#I may need to decide to analyze certain number of users only and the \n#level of interaction for those users could be a criteria de decide this\npercentiles &lt;- quantile(summary_votes_ratio$Ratio_upvotes_per_comment, probs = c(0.25, 0.50, 0.75, 0.90))\n\nprint(percentiles)\n\n      25%       50%       75%       90% \n 1.666667  3.666667 11.000000 31.161905 \n\n\nNow I am working on setting the data base for network analysis."
  },
  {
    "objectID": "posts/Post 3 - Initial Analysis/index.html",
    "href": "posts/Post 3 - Initial Analysis/index.html",
    "title": "Final Project FB - Post 3 - Initial Network Analysis",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\n#library(RedditExtractoR)\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/FelixBetancourt/OneDrive - H-E Parts International/Personal/DACSS-HEP/695N- Network/695NBlog_Felix_Betancourt/posts/Post 3 - Initial Analysis\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\nlength(unique(politik3$url))\n\n[1] 983\n\n\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 31,554 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9         304\n 2 Logical_Parameters           223\n 3 JubalHarshaw23               188\n 4 BrtFrkwr                     178\n 5 Due-Shirt616                 148\n 6 TintedApostle                148\n 7 Alistazia                    142\n 8 LibertyInaFeatherBed         121\n 9 grixorbatz                   118\n10 RickyWinterborn-1080          97\n# ℹ 31,544 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 31,554 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 Jackinapox               15912\n 2 jackleggjr               12829\n 3 tracch                   12462\n 4 OsellusK                 11978\n 5 BlotchComics             11743\n 6 BukkitCrab               11161\n 7 OppositeDifference       10184\n 8 zsreport                  8899\n 9 JubalHarshaw23            8694\n10 AngusMcTibbins            8477\n# ℹ 31,544 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 31,554 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 tracch                                 6231 \n 2 IMissChannel76                         4573 \n 3 TheDudeBeto                            3892 \n 4 Mikraphonechekka12                     3728 \n 5 OokLeeNooma                            2746.\n 6 giddyviewer                            2341 \n 7 pottymcnugg                            2340 \n 8 Fairymask                              2277 \n 9 Manikin_Maker                          2177 \n10 torspice                               2064 \n# ℹ 31,544 more rows\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the df with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a df with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” df which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it sounds still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 posts with a “median” number of comments. One post will be about Trump and another about Biden.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\n\n#Selecting the post for Trump and Biden that we will analyze\n#Let's choose one post for each presidential candidate\n#based on the median number of comments for each\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\nNow I am ready to work on this data set.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(deep = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"deep\"))\nhead(politik_attributes)\n\n# A tibble: 6 × 3\n  score candidate  deep\n  &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n1    20 Biden         1\n2    27 Biden         1\n3     3 Biden         2\n4   -18 Biden         2\n5    23 Biden         3\n6   -12 Biden         4\n\n\nI’ll prepare the adjacency matrix.\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\nglimpse(politik_m)\n\nRows: 119\nColumns: 2\n$ comment_id2 &lt;chr&gt; \"B_2\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B…\n$ author      &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"Angus…\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\nNow let’s explore the Network\n\nsuppressWarnings({\n#load packages\nlibrary(network)\nlibrary(sna)\nlibrary(statnet)\n})\npolitik.n &lt;- network(adj_matrix)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 316 \n    missing edges= 0 \n    non-missing edges= 316 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\n\nSeems that we have 82160 triads in total!\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute\n\n#First I'll create a column with female-male true-false attribute\npolitik_attributes2 &lt;- politik_attributes %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how females and males are interacting\nnodeColors&lt;-ifelse(politik_attributes2$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=T) #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=F) #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")"
  },
  {
    "objectID": "posts/Post 3 - Initial Analysis/index.html#final-project---network-on-subreddit-rpolitics",
    "href": "posts/Post 3 - Initial Analysis/index.html#final-project---network-on-subreddit-rpolitics",
    "title": "Final Project FB - Post 3 - Initial Network Analysis",
    "section": "",
    "text": "I am interested in understanding how social media users influence each other and create communities around specific topics.\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n\n\nIn particular:\n\nHow are Reddit users connected/related in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nIs there a relationship between “upvotes” for a post, number of comments and how it is related to the key users in the network?\n\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\n#library(RedditExtractoR)\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n#politics_reddit &lt;- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments &lt;- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 &lt;- politics_reddit_comments[[1]] \n#politics_list2 &lt;- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n\n\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/FelixBetancourt/OneDrive - H-E Parts International/Personal/DACSS-HEP/695N- Network/695NBlog_Felix_Betancourt/posts/Post 3 - Initial Analysis\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nI’ll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\nlength(unique(politik3$url))\n\n[1] 983\n\n\nLet’s explore a bit about the authors.\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 31,554 × 2\n   author               Total_posts\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Numerous_Photograph9         304\n 2 Logical_Parameters           223\n 3 JubalHarshaw23               188\n 4 BrtFrkwr                     178\n 5 Due-Shirt616                 148\n 6 TintedApostle                148\n 7 Alistazia                    142\n 8 LibertyInaFeatherBed         121\n 9 grixorbatz                   118\n10 RickyWinterborn-1080          97\n# ℹ 31,544 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 31,554 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 Jackinapox               15912\n 2 jackleggjr               12829\n 3 tracch                   12462\n 4 OsellusK                 11978\n 5 BlotchComics             11743\n 6 BukkitCrab               11161\n 7 OppositeDifference       10184\n 8 zsreport                  8899\n 9 JubalHarshaw23            8694\n10 AngusMcTibbins            8477\n# ℹ 31,544 more rows\n\n#Upvotes as a proportion of comments\nsummary_votes_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio &lt;- summary_votes_ratio %&gt;% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n\n# A tibble: 31,554 × 2\n   author             Ratio_upvotes_per_comment\n   &lt;chr&gt;                                  &lt;dbl&gt;\n 1 tracch                                 6231 \n 2 IMissChannel76                         4573 \n 3 TheDudeBeto                            3892 \n 4 Mikraphonechekka12                     3728 \n 5 OokLeeNooma                            2746.\n 6 giddyviewer                            2341 \n 7 pottymcnugg                            2340 \n 8 Fairymask                              2277 \n 9 Manikin_Maker                          2177 \n10 torspice                               2064 \n# ℹ 31,544 more rows\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the df with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a df with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” df which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it sounds still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 posts with a “median” number of comments. One post will be about Trump and another about Biden.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\n\n#Selecting the post for Trump and Biden that we will analyze\n#Let's choose one post for each presidential candidate\n#based on the median number of comments for each\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\nNow I am ready to work on this data set.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(deep = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"deep\"))\nhead(politik_attributes)\n\n# A tibble: 6 × 3\n  score candidate  deep\n  &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n1    20 Biden         1\n2    27 Biden         1\n3     3 Biden         2\n4   -18 Biden         2\n5    23 Biden         3\n6   -12 Biden         4\n\n\nI’ll prepare the adjacency matrix.\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\nglimpse(politik_m)\n\nRows: 119\nColumns: 2\n$ comment_id2 &lt;chr&gt; \"B_2\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B…\n$ author      &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"Angus…\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\nNow let’s explore the Network\n\nsuppressWarnings({\n#load packages\nlibrary(network)\nlibrary(sna)\nlibrary(statnet)\n})\npolitik.n &lt;- network(adj_matrix)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 316 \n    missing edges= 0 \n    non-missing edges= 316 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\n\nSeems that we have 82160 triads in total!\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute\n\n#First I'll create a column with female-male true-false attribute\npolitik_attributes2 &lt;- politik_attributes %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how females and males are interacting\nnodeColors&lt;-ifelse(politik_attributes2$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=T) #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=F) #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")"
  },
  {
    "objectID": "posts/Post 4 - Final Project/index.html",
    "href": "posts/Post 4 - Final Project/index.html",
    "title": "Final Project FB - Post 4 - Final post",
    "section": "",
    "text": "Social media, including Reddit, serves as a prominent platform for discourse and community engagement (Bonneau et al., 2013; Jungherr et al., 2012). The “Politics” subreddit (r/politics) is a hub for discussions on political matters, attracting users who share news articles and express opinions (Bail, 2016).\nIn recent years, Reddit discussions have intensified, especially regarding U.S. politics, notably around Joe Biden and Donald Trump. Analyzing user interactions in r/politics offers insight into digital political landscapes (Conover et al., 2013).\nThis research aims to explore Reddit users’ connections within r/politics, focusing on Biden and Trump discussions. We seek to uncover community formation patterns and influencers through social network analysis. Specifically, we’ll examine users’ connectivity, community emergence, and the relationship between post popularity and user influence.\nBy addressing these questions, we contribute to understanding political discourse on Reddit and social media’s role in shaping public opinion.\n\n\n\n\nHow are Reddit users connected in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nHow are these group of users connected based on the sentiment of their comments?\nIs there a relationship between Score (net value between upvotes and downvotes) for a post, and how the user is connected to other users?\n\n\n\n\nThis research is mainly exploratory, I am not expecting something in particular, so I don’t have an specific hypothesis, except for the research question number 4.\nIn relation to the fourth research question I can expect that:\nH1: an user’s higher score should be highly related to a higher centrality in the network.\n\n\n\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package) and saved the objects generated from the scrapping to csv files\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/FelixBetancourt/OneDrive - H-E Parts International/Personal/DACSS-HEP/695N- Network/695NBlog_Felix_Betancourt/posts/Post 4 - Final Project\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\n#Checking the structure of the data sets\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nFor the purpose of this research we will define the users or authors to posts and comments as the nodes, and edges are defined as comments made in the same post; it does mean that the network will be undirected as I will consider only authors commenting in the same post but I won’t capture the direction of the comment (B is commenting to A post).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nLet’s see how many nodes (authors/users) we got in this data set:\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the data set with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a data set with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” data set which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it is still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 specific posts with a “median” number of comments. One post will be about Trump and another about Biden, specifically I will filter posts by containing the word “Biden” and “Trump” in the title of the post.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n\n\nThe median comment for Trump’s post is 74 comments.Therefore I’ll select the post with 74 comments.\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\nNow let’s select Biden’s post\n\nsuppressWarnings({\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\nMedian is 66.5 comments, let’s use the post with 67 comments.\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\n\n\nBefore proceeding with the network analysis, let’s explore a bit about the authors (users).\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_final2 &lt;- politik_final2 %&gt;% mutate(countid = \"1\")\npolitik_final2$countid &lt;- as.numeric(politik_final2$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_final2)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 80 × 2\n   author            Total_posts\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 BrtFrkwr                    7\n 2 betterwoke                  5\n 3 Hattopia                    4\n 4 TheBodyPolitic1             4\n 5 TheRandomInteger            4\n 6 AngusMcTibbins              3\n 7 Due-Shirt616                3\n 8 NoDesinformatziya           3\n 9 TheReddestOrange            3\n10 DarkwingDuckHunt            2\n# ℹ 70 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 80 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 r-m-russell                 76\n 2 AngusMcTibbins              68\n 3 OverlyComplexPants          40\n 4 BrtFrkwr                    33\n 5 betterwoke                  28\n 6 TheBodyPolitic1             24\n 7 penis_berry_crunch          22\n 8 Due-Shirt616                21\n 9 NoDesinformatziya           20\n10 YeaterdaysQuim              20\n# ℹ 70 more rows\n\n#Score as a proportion of comments\nsummary_score_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_score_per_comment = sum(score)/sum(countid))\nsummary_score_ratio &lt;- summary_score_ratio %&gt;% arrange(desc(Ratio_score_per_comment))\nprint(summary_score_ratio)\n\n# A tibble: 80 × 2\n   author             Ratio_score_per_comment\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 r-m-russell                           76  \n 2 OverlyComplexPants                    40  \n 3 AngusMcTibbins                        22.7\n 4 penis_berry_crunch                    22  \n 5 YeaterdaysQuim                        20  \n 6 fxkatt                                20  \n 7 grixorbatz                            17  \n 8 Sea_Engine4333                        16  \n 9 Quiet_Dimensions                      14  \n10 hdiggyh                               14  \n# ℹ 70 more rows\n\n\nI would expect that nodes (users) with higher score and/or higher score per comment should be predominant (central) in the network.\nFor instance we got the following users in the top 5 in terms of score: “r-m-russell”, “AngusMcTibbins”, “OverlyComplexPants”, “BrtFrkwr” and “betterwoke”.\nOn the other hand here the top 5 users with highest score per comment: “r-m-russell”, “OverlyComplexPants”, “AngusMcTibbins”, “penis_berry_crunch”, and “YeaterdaysQuim”.\nNow I am ready to work on this data set for the Network analysis.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(distance = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"distance\"))\n\nI’ll prepare the adjacency matrix:\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\n\n\nNow let’s explore the Network.\nIt is important to mention that in this research we will assume an undirected network. We are only considering comments within the same post, not specific “direction” of each comment among users.\n\n#load packages\nsuppressPackageStartupMessages(library(network))\nlibrary(sna)\nlibrary(statnet)\n\npolitik.n &lt;- network(adj_matrix, directed = FALSE)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 158 \n    missing edges= 0 \n    non-missing edges= 158 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\nLet’s explore the network. I’ll calculate the census for Dyads and triads:\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\n\nIn terms of Dyads, we got 158 mutual connections and 3002 null connections.\nIn terms of Triads, we got 82160 triads in total: about 70k null triads, 11k open triads (one connection exist), 179 where 2 connections exist, and 337 closed triads.\nIt seems a disperse network, but let’s see Transitivity and Density:\nLet’s check the Transitivity coefficient\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density:\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes and labels\n\n# Plot the network\nplot(politik.n, displaylabels = F, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute. We will clasify the users as per their comments to Biden or Trump posts.\n\n#I'll create a column with Biden true-false attribute\npolitik_final3at &lt;- politik_final3 %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how authonrs in Biden and Trump are interacting\nnodeColors&lt;-ifelse(politik_final3at$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=T, main = \"Authors Network by Candidate\") #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nLet’s exclude isolated nodes\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=F, main = \"Authors Network by Candidate (excluding isolated nodes)\") #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nIt looks like there are closed communities not connected among them. Which is consistent with the Transitivity vs Density finding before.\nIn particular Biden’s commentors tend to be together and Trump’s commentors seems more disperse.\nLet’s see who are the authors with highest degree centrality\n\n# create a dataset of vertex names and degree: statnet\npolitik.nodes.df &lt;- data.frame(name = politik.n %v% \"vertex.names\",\n                            degree = sna::degree(politik.n))\n\npolitik_table7 &lt;- data.table(politik.nodes.df)\n\n#order by centrality degree\npolitik_table7 %&gt;% arrange(desc(degree)) %&gt;%\nslice(1:10)\n\n                  name degree\n                &lt;char&gt;  &lt;num&gt;\n 1:           Knoxcore     30\n 2:  NoDesinformatziya     30\n 3:           BrtFrkwr     28\n 4:   TheRandomInteger     26\n 5:    TheBodyPolitic1     20\n 6:         Fasefirst2     20\n 7:           cdiddy19     20\n 8: Invincible_auxcord     20\n 9:     Bulky-You-5657     20\n10:        nickmiele22     20\n\nsummary(politik_table7)\n\n     name               degree    \n Length:80          Min.   : 0.0  \n Class :character   1st Qu.: 0.0  \n Mode  :character   Median : 6.0  \n                    Mean   : 7.9  \n                    3rd Qu.:14.5  \n                    Max.   :30.0  \n\n\nLet’s explore the correlation between centrality degree and score (remember it is the net value between upvotes and downvotes).\nI am expecting a significant positive relationship between soore and degree centrality.\n\nsuppressPackageStartupMessages(library(igraph))\n\n# Create the igraph object\npolitik.ig &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")  # Undirected by default\n\n# Calculate degree centrality for each node\ndegree_centrality &lt;- degree(politik.ig, mode = \"all\")\n\n# If nodes do not have names, you can use node IDs\nif (is.null(V(politik.ig)$name)) {\n  V(politik.ig)$name &lt;- as.character(1:vcount(politik.ig))\n}\n\n# Check node names\nnode_names &lt;- V(politik.ig)$name\n\n# Create a sample data frame with some values for each node\n# Ensure the data frame has the same node identifiers as the graph\ndf &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  value = runif(vcount(politik.ig), 1, 100)  # Random values between 1 and 100\n)\n\n# Convert degree centrality to a data frame\ndegree_centrality_df &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  degree_centrality = degree_centrality  # Degree centrality values\n)\n\n# Merge the degree centrality data frame with the existing data frame\nmerged_df &lt;- merge(df, degree_centrality_df, by = \"author\", all = TRUE)\nmerged_df2 &lt;- merge(merged_df, politik_final2, by = \"author\", all = TRUE)\ndegree_scoredf &lt;- select(merged_df2, c(\"degree_centrality\", \"score\"))\n\n# Calculate the correlation coefficient between 'score' and 'degree_centrality'\ncor_matrix1 &lt;- cor(degree_scoredf, use = \"complete.obs\")\ncor_matrix1\n\n                  degree_centrality       score\ndegree_centrality       1.000000000 0.001921923\nscore                   0.001921923 1.000000000\n\n\nSeems that the relationship between score and degree centrality is very low (0.002).\nSo I can’t accept my hypothesis.\nI am curious about the top 5 users with higher degree centrality.\n\ndegree_scoredf3 &lt;- select(merged_df2, c(\"degree_centrality\", \"author\"))\nsubset_df &lt;- distinct(degree_scoredf3, author, .keep_all = TRUE)\n\nsubset_df %&gt;% arrange(desc(degree_centrality))\n\n   degree_centrality               author\n1                 15             Knoxcore\n2                 15    NoDesinformatziya\n3                 14             BrtFrkwr\n4                 13     TheRandomInteger\n5                 10       Bulky-You-5657\n6                 10             cdiddy19\n7                 10  Exciting_Slice_9492\n8                 10           Fasefirst2\n9                 10        hilljack26301\n10                10   Invincible_auxcord\n11                10             Kamelasa\n12                10          nickmiele22\n13                10      TheBodyPolitic1\n14                 8   Admirable_Bad_5649\n15                 8           betterwoke\n16                 8     DarkwingDuckHunt\n17                 8          Scarlettail\n18                 8        Spara-Extreme\n19                 8     TheReddestOrange\n20                 8     Thick-Return1694\n21                 7          bakeacake45\n22                 6            AliMcGraw\n23                 6       AngusMcTibbins\n24                 6             Hattopia\n25                 6    hellocattlecookie\n26                 6   Mundane_Rabbit7751\n27                 6   OverlyComplexPants\n28                 6         Roasted_Butt\n29                 6    SeegsonSynthetics\n30                 6             Shaunair\n31                 4             caserock\n32                 4     CecilTWashington\n33                 4            gefjunhel\n34                 4   penis_berry_crunch\n35                 4          r-m-russell\n36                 3         Due-Shirt616\n37                 3              hdiggyh\n38                 3             iuthnj34\n39                 3               JeffMo\n40                 3        PineTreeBanjo\n41                 3              Purify5\n42                 3     Quiet_Dimensions\n43                 3       YeaterdaysQuim\n44                 1  Candid_Chicken_9246\n45                 1   FijiFanBotNotGay69\n46                 1        Happypappy213\n47                 1            hindusoul\n48                 1      InGreedWeTrust3\n49                 1              LariRed\n50                 1   physical_graffitti\n51                 1            Tower6011\n52                 0              bck1999\n53                 0     bloombergopinion\n54                 0          cryolongman\n55                 0          Donut131313\n56                 0              eldred2\n57                 0      Empty-Rise-4409\n58                 0     fore_skin_walker\n59                 0               fxkatt\n60                 0           grixorbatz\n61                 0            Guttenber\n62                 0        HonoredPeople\n63                 0     ImportantNeck491\n64                 0        inconsistent3\n65                 0       JubalHarshaw23\n66                 0             njman100\n67                 0          No_Yak_6227\n68                 0       Odd_Tiger_2278\n69                 0 platanthera_ciliaris\n70                 0        PoopieButt317\n71                 0  Practical_Shop_4055\n72                 0         RUIN_NATION_\n73                 0       Sea_Engine4333\n74                 0          SeaSuch2077\n75                 0             spotspam\n76                 0          stjoechief1\n77                 0          StormOk7544\n78                 0              syg-123\n79                 0               th1961\n80                 0         Willowgirl78\n\n\nUsers with higher degree centrality are “Knoxcore”, “NoDesinformatziya”, “BrtFrkwr”, “TheRandomInteger”, and “Bulky-You-5657”\nOut of these 5 nodes only one of them (“BrtFrkwr”) is also in the top 5 related to score. So it is consistent with the low correlation between score and centrality.\nLet’s now check what clusters (communities) we do have in this network.\n\n# run clustering algorithm: fast_greedy\npolitik.fg &lt;- igraph::cluster_fast_greedy(politik.ig)\n# inspect clustering object\npolitik.fg\n\nIGRAPH clustering fast greedy, groups: 37, mod: 0.63\n+ groups:\n  $`1`\n   [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n   [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n   [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n  [10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n  [13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n  [16] \"Kamelasa\"           \n  \n  $`2`\n   [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n  + ... omitted several groups/vertices\n\nigraph::groups(politik.fg)\n\n$`1`\n [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n[10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n[13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n[16] \"Kamelasa\"           \n\n$`2`\n [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n [4] \"Roasted_Butt\"        \"BrtFrkwr\"            \"TheRandomInteger\"   \n [7] \"Shaunair\"            \"betterwoke\"          \"DarkwingDuckHunt\"   \n[10] \"Thick-Return1694\"    \"Spara-Extreme\"       \"Admirable_Bad_5649\" \n[13] \"TheReddestOrange\"    \"Scarlettail\"         \"Tower6011\"          \n[16] \"Candid_Chicken_9246\"\n\n$`3`\n[1] \"r-m-russell\"        \"penis_berry_crunch\" \"CecilTWashington\"  \n[4] \"caserock\"           \"gefjunhel\"         \n\n$`4`\n[1] \"iuthnj34\"       \"YeaterdaysQuim\" \"hdiggyh\"        \"Purify5\"       \n\n$`5`\n[1] \"Quiet_Dimensions\" \"Due-Shirt616\"     \"JeffMo\"           \"PineTreeBanjo\"   \n\n$`6`\n[1] \"physical_graffitti\" \"hindusoul\"         \n\n$`7`\n[1] \"LariRed\"         \"InGreedWeTrust3\"\n\n$`8`\n[1] \"Happypappy213\"      \"FijiFanBotNotGay69\"\n\n$`9`\n[1] \"fxkatt\"\n\n$`10`\n[1] \"Sea_Engine4333\"\n\n$`11`\n[1] \"Practical_Shop_4055\"\n\n$`12`\n[1] \"PoopieButt317\"\n\n$`13`\n[1] \"inconsistent3\"\n\n$`14`\n[1] \"Empty-Rise-4409\"\n\n$`15`\n[1] \"stjoechief1\"\n\n$`16`\n[1] \"th1961\"\n\n$`17`\n[1] \"platanthera_ciliaris\"\n\n$`18`\n[1] \"No_Yak_6227\"\n\n$`19`\n[1] \"fore_skin_walker\"\n\n$`20`\n[1] \"HonoredPeople\"\n\n$`21`\n[1] \"bloombergopinion\"\n\n$`22`\n[1] \"RUIN_NATION_\"\n\n$`23`\n[1] \"ImportantNeck491\"\n\n$`24`\n[1] \"grixorbatz\"\n\n$`25`\n[1] \"spotspam\"\n\n$`26`\n[1] \"JubalHarshaw23\"\n\n$`27`\n[1] \"StormOk7544\"\n\n$`28`\n[1] \"njman100\"\n\n$`29`\n[1] \"Odd_Tiger_2278\"\n\n$`30`\n[1] \"cryolongman\"\n\n$`31`\n[1] \"Willowgirl78\"\n\n$`32`\n[1] \"Guttenber\"\n\n$`33`\n[1] \"SeaSuch2077\"\n\n$`34`\n[1] \"syg-123\"\n\n$`35`\n[1] \"bck1999\"\n\n$`36`\n[1] \"Donut131313\"\n\n$`37`\n[1] \"eldred2\"\n\n\nThere are 2 main clusters in the network.\nLet’s see the density of each cluster using block model function.\n\nprint(blockmodel(politik.n, politik.fg$membership)$block.model,\n      digits = 2)\n\n         Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8\nBlock 1     0.62    0.00       0       0       0       0       0       0\nBlock 2     0.00    0.48       0       0       0       0       0       0\nBlock 3     0.00    0.00       1       0       0       0       0       0\nBlock 4     0.00    0.00       0       1       0       0       0       0\nBlock 5     0.00    0.00       0       0       1       0       0       0\nBlock 6     0.00    0.00       0       0       0       1       0       0\nBlock 7     0.00    0.00       0       0       0       0       1       0\nBlock 8     0.00    0.00       0       0       0       0       0       1\nBlock 9     0.00    0.00       0       0       0       0       0       0\nBlock 10    0.00    0.00       0       0       0       0       0       0\nBlock 11    0.00    0.00       0       0       0       0       0       0\nBlock 12    0.00    0.00       0       0       0       0       0       0\nBlock 13    0.00    0.00       0       0       0       0       0       0\nBlock 14    0.00    0.00       0       0       0       0       0       0\nBlock 15    0.00    0.00       0       0       0       0       0       0\nBlock 16    0.00    0.00       0       0       0       0       0       0\nBlock 17    0.00    0.00       0       0       0       0       0       0\nBlock 18    0.00    0.00       0       0       0       0       0       0\nBlock 19    0.00    0.00       0       0       0       0       0       0\nBlock 20    0.00    0.00       0       0       0       0       0       0\nBlock 21    0.00    0.00       0       0       0       0       0       0\nBlock 22    0.00    0.00       0       0       0       0       0       0\nBlock 23    0.00    0.00       0       0       0       0       0       0\nBlock 24    0.00    0.00       0       0       0       0       0       0\nBlock 25    0.00    0.00       0       0       0       0       0       0\nBlock 26    0.00    0.00       0       0       0       0       0       0\nBlock 27    0.00    0.00       0       0       0       0       0       0\nBlock 28    0.00    0.00       0       0       0       0       0       0\nBlock 29    0.00    0.00       0       0       0       0       0       0\nBlock 30    0.00    0.00       0       0       0       0       0       0\nBlock 31    0.00    0.00       0       0       0       0       0       0\nBlock 32    0.00    0.00       0       0       0       0       0       0\nBlock 33    0.00    0.00       0       0       0       0       0       0\nBlock 34    0.00    0.00       0       0       0       0       0       0\nBlock 35    0.00    0.00       0       0       0       0       0       0\nBlock 36    0.00    0.00       0       0       0       0       0       0\nBlock 37    0.00    0.00       0       0       0       0       0       0\n         Block 9 Block 10 Block 11 Block 12 Block 13 Block 14 Block 15 Block 16\nBlock 1        0        0        0        0        0        0        0        0\nBlock 2        0        0        0        0        0        0        0        0\nBlock 3        0        0        0        0        0        0        0        0\nBlock 4        0        0        0        0        0        0        0        0\nBlock 5        0        0        0        0        0        0        0        0\nBlock 6        0        0        0        0        0        0        0        0\nBlock 7        0        0        0        0        0        0        0        0\nBlock 8        0        0        0        0        0        0        0        0\nBlock 9      NaN        0        0        0        0        0        0        0\nBlock 10       0      NaN        0        0        0        0        0        0\nBlock 11       0        0      NaN        0        0        0        0        0\nBlock 12       0        0        0      NaN        0        0        0        0\nBlock 13       0        0        0        0      NaN        0        0        0\nBlock 14       0        0        0        0        0      NaN        0        0\nBlock 15       0        0        0        0        0        0      NaN        0\nBlock 16       0        0        0        0        0        0        0      NaN\nBlock 17       0        0        0        0        0        0        0        0\nBlock 18       0        0        0        0        0        0        0        0\nBlock 19       0        0        0        0        0        0        0        0\nBlock 20       0        0        0        0        0        0        0        0\nBlock 21       0        0        0        0        0        0        0        0\nBlock 22       0        0        0        0        0        0        0        0\nBlock 23       0        0        0        0        0        0        0        0\nBlock 24       0        0        0        0        0        0        0        0\nBlock 25       0        0        0        0        0        0        0        0\nBlock 26       0        0        0        0        0        0        0        0\nBlock 27       0        0        0        0        0        0        0        0\nBlock 28       0        0        0        0        0        0        0        0\nBlock 29       0        0        0        0        0        0        0        0\nBlock 30       0        0        0        0        0        0        0        0\nBlock 31       0        0        0        0        0        0        0        0\nBlock 32       0        0        0        0        0        0        0        0\nBlock 33       0        0        0        0        0        0        0        0\nBlock 34       0        0        0        0        0        0        0        0\nBlock 35       0        0        0        0        0        0        0        0\nBlock 36       0        0        0        0        0        0        0        0\nBlock 37       0        0        0        0        0        0        0        0\n         Block 17 Block 18 Block 19 Block 20 Block 21 Block 22 Block 23\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17      NaN        0        0        0        0        0        0\nBlock 18        0      NaN        0        0        0        0        0\nBlock 19        0        0      NaN        0        0        0        0\nBlock 20        0        0        0      NaN        0        0        0\nBlock 21        0        0        0        0      NaN        0        0\nBlock 22        0        0        0        0        0      NaN        0\nBlock 23        0        0        0        0        0        0      NaN\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 24 Block 25 Block 26 Block 27 Block 28 Block 29 Block 30\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24      NaN        0        0        0        0        0        0\nBlock 25        0      NaN        0        0        0        0        0\nBlock 26        0        0      NaN        0        0        0        0\nBlock 27        0        0        0      NaN        0        0        0\nBlock 28        0        0        0        0      NaN        0        0\nBlock 29        0        0        0        0        0      NaN        0\nBlock 30        0        0        0        0        0        0      NaN\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 31 Block 32 Block 33 Block 34 Block 35 Block 36 Block 37\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31      NaN        0        0        0        0        0        0\nBlock 32        0      NaN        0        0        0        0        0\nBlock 33        0        0      NaN        0        0        0        0\nBlock 34        0        0        0      NaN        0        0        0\nBlock 35        0        0        0        0      NaN        0        0\nBlock 36        0        0        0        0        0      NaN        0\nBlock 37        0        0        0        0        0        0      NaN\n\n\nThe blocks 1 and 2 in our network seems dense.\n\ndf_comm &lt;- data.frame(\n  Node = V(politik.ig)$name,\n  Community = politik.fg$membership,\n  Degree = degree_centrality\n)\n\n# 4. Find maximum degree for each community\n\nhighest_degree_nodes &lt;- df_comm %&gt;%\n  group_by(Community) %&gt;%\n  filter(Degree == max(Degree)) %&gt;%\n  ungroup()\n\nhighest_degree_nodes %&gt;% arrange(-desc(Community))\n\n# A tibble: 51 × 3\n   Node               Community Degree\n   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Knoxcore                   1     15\n 2 NoDesinformatziya          1     15\n 3 BrtFrkwr                   2     14\n 4 r-m-russell                3      4\n 5 penis_berry_crunch         3      4\n 6 CecilTWashington           3      4\n 7 caserock                   3      4\n 8 gefjunhel                  3      4\n 9 iuthnj34                   4      3\n10 YeaterdaysQuim             4      3\n# ℹ 41 more rows\n\n\nLet’s now visualize the communities including the nodes with highest degree centrality for the main communities (1 and 2):\n\n# Identify nodes with high degree centrality (e.g., top 3)\ntop_nodes &lt;- names(sort(degree_centrality, decreasing = TRUE))[1:5]\n\n# Create a custom labeling vector\nnode_labels &lt;- rep(NA, vcount(politik.ig))\n# Set the labels for the nodes with high degree centrality\nnode_labels[top_nodes] &lt;- top_nodes\n\n# Identify isolated nodes\nisolated_nodes &lt;- which(degree(politik.ig) == 0)\n\n# Remove isolated nodes from the network\nnetwork_no_isolated &lt;- delete_vertices(politik.ig, isolated_nodes)\n\nplot(network_no_isolated,\n     vertex.label = node_labels,  \n     vertex.label.cex = 0.8,\n     vertex.color = membership(politik.fg),\n     vertex.label.color = \"black\",\n     vertex.shape = \"sphere\",\n     layout = layout_with_fr,\n     main = \"Communities without isolated nodes and labeling top 5 central nodes\")\n\n\n\n\n\n\n\n\nIt looks like while we have 2 main communities the nodes with highest degree centrality are mostly in one of the communities.\nIt seems like that group of authors are close among them and at the same time are central in the network.\n\n\n\nLastly let’s add a new attribute to the network using text analysis, in particular sentiment analysis.\nLet’s get the sentiment for each author’s comments.\n\nlibrary(sentimentr)\n\n#labeling based on the sentiment score\ncomments &lt;- politik_final3$comment\nget_sentiment_label &lt;- function(ave_sentiment) {\n  if (ave_sentiment &gt; 0.1) {\n    return(\"Positive\")\n  } else if (ave_sentiment &lt; -0.1) {\n    return(\"Negative\")\n  } else {\n    return(\"Neutral\")\n  }\n}\nsentiment_scores &lt;- sentiment_by(x = comments, text.var = comments)\n\n#adding the label for each author in the data set\npolitik_final3$sentiment &lt;- sapply(sentiment_scores$ave_sentiment, get_sentiment_label)\n\nNow let’s visualize the network by sentiment of each node.\n\n# Create a graph object from the data frame\ng &lt;- graph_from_data_frame(politik.n, directed = FALSE)\n\n# Add node attributes to the graph\nV(g)$sentiment &lt;- politik_final3$sentiment[match(V(g)$name, politik_final3$author)]\n\n\n# Define color palette for categories\ncolor_palette &lt;- c(\"Negative\" = \"red\", \"Neutral\" = \"blue\", \"Positive\" = \"lightgreen\")\n\n# Visualize the network with colored nodes\nplot(g, vertex.color = color_palette[V(g)$sentiment], layout = layout_nicely, vertex.label = NA, vertex.size = 7, isolates=TRUE, main = \"Authors Network by Sentiment\")\nlegend(\"bottomright\", legend = c(\"Positive\", \"Neutral\", \"Negative\"), col = c(\"lightgreen\", \"blue\", \"red\"), pch = c(21, 21), title = \"Node Sentiment\")\n\n\n\n\n\n\n\n\nFrom this chart it looks like:\n\nMost of the sentiments are either neutral or negative\nSentiments tend to get closer, or group among them.\n\n\n\n\n\n\n\nThe network was found to be sparse (low density) but highly interconnected within subgroups (high transitivity), suggesting the presence of distinct communities.\nThe visualization of the network confirmed the presence of communities, showing separate clusters of users commenting on Biden and Trump posts. The network was divided into 37 distinct clusters, indicating multiple smaller communities within the larger Biden and Trump-focused groups.\nThe hypothesis that users with higher scores (more upvotes or higher score) would also have more central positions in the network was not supported. The correlation between score and centrality was negligible.\nWhile score wasn’t a strong indicator of influence, degree centrality (number of connections) was used to identify the most connected users. These users may play important roles in shaping discussions within their respective communities.\nIn terms of the sentiment analysis it seems that most comments were neutral or negative. And, users with similar sentiments tended to interact with each other.\n\nThis Network Analysis research offers interesting insights into the social dynamics of a politically charged online community. It highlights the presence of distinct communities, the complex interplay between user engagement (score) and influence (centrality), and the opportunity for further exploration of the factors shaping political discourse on platforms like Reddit."
  },
  {
    "objectID": "posts/Post 4 - Final Project/index.html#final-project-695n---network-on-subreddit-rpolitics",
    "href": "posts/Post 4 - Final Project/index.html#final-project-695n---network-on-subreddit-rpolitics",
    "title": "Final Project FB - Post 4 - Final post",
    "section": "",
    "text": "Social media, including Reddit, serves as a prominent platform for discourse and community engagement (Bonneau et al., 2013; Jungherr et al., 2012). The “Politics” subreddit (r/politics) is a hub for discussions on political matters, attracting users who share news articles and express opinions (Bail, 2016).\nIn recent years, Reddit discussions have intensified, especially regarding U.S. politics, notably around Joe Biden and Donald Trump. Analyzing user interactions in r/politics offers insight into digital political landscapes (Conover et al., 2013).\nThis research aims to explore Reddit users’ connections within r/politics, focusing on Biden and Trump discussions. We seek to uncover community formation patterns and influencers through social network analysis. Specifically, we’ll examine users’ connectivity, community emergence, and the relationship between post popularity and user influence.\nBy addressing these questions, we contribute to understanding political discourse on Reddit and social media’s role in shaping public opinion.\n\n\n\n\nHow are Reddit users connected in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nHow are these group of users connected based on the sentiment of their comments?\nIs there a relationship between Score (net value between upvotes and downvotes) for a post, and how the user is connected to other users?\n\n\n\n\nThis research is mainly exploratory, I am not expecting something in particular, so I don’t have an specific hypothesis, except for the research question number 4.\nIn relation to the fourth research question I can expect that:\nH1: an user’s higher score should be highly related to a higher centrality in the network.\n\n\n\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\n\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package) and saved the objects generated from the scrapping to csv files\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/FelixBetancourt/OneDrive - H-E Parts International/Personal/DACSS-HEP/695N- Network/695NBlog_Felix_Betancourt/posts/Post 4 - Final Project\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\n#Checking the structure of the data sets\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nFor the purpose of this research we will define the users or authors to posts and comments as the nodes, and edges are defined as comments made in the same post; it does mean that the network will be undirected as I will consider only authors commenting in the same post but I won’t capture the direction of the comment (B is commenting to A post).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nLet’s see how many nodes (authors/users) we got in this data set:\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the data set with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a data set with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” data set which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it is still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 specific posts with a “median” number of comments. One post will be about Trump and another about Biden, specifically I will filter posts by containing the word “Biden” and “Trump” in the title of the post.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n\n\nThe median comment for Trump’s post is 74 comments.Therefore I’ll select the post with 74 comments.\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\nNow let’s select Biden’s post\n\nsuppressWarnings({\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\nMedian is 66.5 comments, let’s use the post with 67 comments.\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\n\n\nBefore proceeding with the network analysis, let’s explore a bit about the authors (users).\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_final2 &lt;- politik_final2 %&gt;% mutate(countid = \"1\")\npolitik_final2$countid &lt;- as.numeric(politik_final2$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_final2)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 80 × 2\n   author            Total_posts\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 BrtFrkwr                    7\n 2 betterwoke                  5\n 3 Hattopia                    4\n 4 TheBodyPolitic1             4\n 5 TheRandomInteger            4\n 6 AngusMcTibbins              3\n 7 Due-Shirt616                3\n 8 NoDesinformatziya           3\n 9 TheReddestOrange            3\n10 DarkwingDuckHunt            2\n# ℹ 70 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 80 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 r-m-russell                 76\n 2 AngusMcTibbins              68\n 3 OverlyComplexPants          40\n 4 BrtFrkwr                    33\n 5 betterwoke                  28\n 6 TheBodyPolitic1             24\n 7 penis_berry_crunch          22\n 8 Due-Shirt616                21\n 9 NoDesinformatziya           20\n10 YeaterdaysQuim              20\n# ℹ 70 more rows\n\n#Score as a proportion of comments\nsummary_score_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_score_per_comment = sum(score)/sum(countid))\nsummary_score_ratio &lt;- summary_score_ratio %&gt;% arrange(desc(Ratio_score_per_comment))\nprint(summary_score_ratio)\n\n# A tibble: 80 × 2\n   author             Ratio_score_per_comment\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 r-m-russell                           76  \n 2 OverlyComplexPants                    40  \n 3 AngusMcTibbins                        22.7\n 4 penis_berry_crunch                    22  \n 5 YeaterdaysQuim                        20  \n 6 fxkatt                                20  \n 7 grixorbatz                            17  \n 8 Sea_Engine4333                        16  \n 9 Quiet_Dimensions                      14  \n10 hdiggyh                               14  \n# ℹ 70 more rows\n\n\nI would expect that nodes (users) with higher score and/or higher score per comment should be predominant (central) in the network.\nFor instance we got the following users in the top 5 in terms of score: “r-m-russell”, “AngusMcTibbins”, “OverlyComplexPants”, “BrtFrkwr” and “betterwoke”.\nOn the other hand here the top 5 users with highest score per comment: “r-m-russell”, “OverlyComplexPants”, “AngusMcTibbins”, “penis_berry_crunch”, and “YeaterdaysQuim”.\nNow I am ready to work on this data set for the Network analysis.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(distance = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"distance\"))\n\nI’ll prepare the adjacency matrix:\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\n\n\nNow let’s explore the Network.\nIt is important to mention that in this research we will assume an undirected network. We are only considering comments within the same post, not specific “direction” of each comment among users.\n\n#load packages\nsuppressPackageStartupMessages(library(network))\nlibrary(sna)\nlibrary(statnet)\n\npolitik.n &lt;- network(adj_matrix, directed = FALSE)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 158 \n    missing edges= 0 \n    non-missing edges= 158 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\nLet’s explore the network. I’ll calculate the census for Dyads and triads:\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\n\nIn terms of Dyads, we got 158 mutual connections and 3002 null connections.\nIn terms of Triads, we got 82160 triads in total: about 70k null triads, 11k open triads (one connection exist), 179 where 2 connections exist, and 337 closed triads.\nIt seems a disperse network, but let’s see Transitivity and Density:\nLet’s check the Transitivity coefficient\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density:\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes and labels\n\n# Plot the network\nplot(politik.n, displaylabels = F, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute. We will clasify the users as per their comments to Biden or Trump posts.\n\n#I'll create a column with Biden true-false attribute\npolitik_final3at &lt;- politik_final3 %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how authonrs in Biden and Trump are interacting\nnodeColors&lt;-ifelse(politik_final3at$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=T, main = \"Authors Network by Candidate\") #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nLet’s exclude isolated nodes\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=F, main = \"Authors Network by Candidate (excluding isolated nodes)\") #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nIt looks like there are closed communities not connected among them. Which is consistent with the Transitivity vs Density finding before.\nIn particular Biden’s commentors tend to be together and Trump’s commentors seems more disperse.\nLet’s see who are the authors with highest degree centrality\n\n# create a dataset of vertex names and degree: statnet\npolitik.nodes.df &lt;- data.frame(name = politik.n %v% \"vertex.names\",\n                            degree = sna::degree(politik.n))\n\npolitik_table7 &lt;- data.table(politik.nodes.df)\n\n#order by centrality degree\npolitik_table7 %&gt;% arrange(desc(degree)) %&gt;%\nslice(1:10)\n\n                  name degree\n                &lt;char&gt;  &lt;num&gt;\n 1:           Knoxcore     30\n 2:  NoDesinformatziya     30\n 3:           BrtFrkwr     28\n 4:   TheRandomInteger     26\n 5:    TheBodyPolitic1     20\n 6:         Fasefirst2     20\n 7:           cdiddy19     20\n 8: Invincible_auxcord     20\n 9:     Bulky-You-5657     20\n10:        nickmiele22     20\n\nsummary(politik_table7)\n\n     name               degree    \n Length:80          Min.   : 0.0  \n Class :character   1st Qu.: 0.0  \n Mode  :character   Median : 6.0  \n                    Mean   : 7.9  \n                    3rd Qu.:14.5  \n                    Max.   :30.0  \n\n\nLet’s explore the correlation between centrality degree and score (remember it is the net value between upvotes and downvotes).\nI am expecting a significant positive relationship between soore and degree centrality.\n\nsuppressPackageStartupMessages(library(igraph))\n\n# Create the igraph object\npolitik.ig &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")  # Undirected by default\n\n# Calculate degree centrality for each node\ndegree_centrality &lt;- degree(politik.ig, mode = \"all\")\n\n# If nodes do not have names, you can use node IDs\nif (is.null(V(politik.ig)$name)) {\n  V(politik.ig)$name &lt;- as.character(1:vcount(politik.ig))\n}\n\n# Check node names\nnode_names &lt;- V(politik.ig)$name\n\n# Create a sample data frame with some values for each node\n# Ensure the data frame has the same node identifiers as the graph\ndf &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  value = runif(vcount(politik.ig), 1, 100)  # Random values between 1 and 100\n)\n\n# Convert degree centrality to a data frame\ndegree_centrality_df &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  degree_centrality = degree_centrality  # Degree centrality values\n)\n\n# Merge the degree centrality data frame with the existing data frame\nmerged_df &lt;- merge(df, degree_centrality_df, by = \"author\", all = TRUE)\nmerged_df2 &lt;- merge(merged_df, politik_final2, by = \"author\", all = TRUE)\ndegree_scoredf &lt;- select(merged_df2, c(\"degree_centrality\", \"score\"))\n\n# Calculate the correlation coefficient between 'score' and 'degree_centrality'\ncor_matrix1 &lt;- cor(degree_scoredf, use = \"complete.obs\")\ncor_matrix1\n\n                  degree_centrality       score\ndegree_centrality       1.000000000 0.001921923\nscore                   0.001921923 1.000000000\n\n\nSeems that the relationship between score and degree centrality is very low (0.002).\nSo I can’t accept my hypothesis.\nI am curious about the top 5 users with higher degree centrality.\n\ndegree_scoredf3 &lt;- select(merged_df2, c(\"degree_centrality\", \"author\"))\nsubset_df &lt;- distinct(degree_scoredf3, author, .keep_all = TRUE)\n\nsubset_df %&gt;% arrange(desc(degree_centrality))\n\n   degree_centrality               author\n1                 15             Knoxcore\n2                 15    NoDesinformatziya\n3                 14             BrtFrkwr\n4                 13     TheRandomInteger\n5                 10       Bulky-You-5657\n6                 10             cdiddy19\n7                 10  Exciting_Slice_9492\n8                 10           Fasefirst2\n9                 10        hilljack26301\n10                10   Invincible_auxcord\n11                10             Kamelasa\n12                10          nickmiele22\n13                10      TheBodyPolitic1\n14                 8   Admirable_Bad_5649\n15                 8           betterwoke\n16                 8     DarkwingDuckHunt\n17                 8          Scarlettail\n18                 8        Spara-Extreme\n19                 8     TheReddestOrange\n20                 8     Thick-Return1694\n21                 7          bakeacake45\n22                 6            AliMcGraw\n23                 6       AngusMcTibbins\n24                 6             Hattopia\n25                 6    hellocattlecookie\n26                 6   Mundane_Rabbit7751\n27                 6   OverlyComplexPants\n28                 6         Roasted_Butt\n29                 6    SeegsonSynthetics\n30                 6             Shaunair\n31                 4             caserock\n32                 4     CecilTWashington\n33                 4            gefjunhel\n34                 4   penis_berry_crunch\n35                 4          r-m-russell\n36                 3         Due-Shirt616\n37                 3              hdiggyh\n38                 3             iuthnj34\n39                 3               JeffMo\n40                 3        PineTreeBanjo\n41                 3              Purify5\n42                 3     Quiet_Dimensions\n43                 3       YeaterdaysQuim\n44                 1  Candid_Chicken_9246\n45                 1   FijiFanBotNotGay69\n46                 1        Happypappy213\n47                 1            hindusoul\n48                 1      InGreedWeTrust3\n49                 1              LariRed\n50                 1   physical_graffitti\n51                 1            Tower6011\n52                 0              bck1999\n53                 0     bloombergopinion\n54                 0          cryolongman\n55                 0          Donut131313\n56                 0              eldred2\n57                 0      Empty-Rise-4409\n58                 0     fore_skin_walker\n59                 0               fxkatt\n60                 0           grixorbatz\n61                 0            Guttenber\n62                 0        HonoredPeople\n63                 0     ImportantNeck491\n64                 0        inconsistent3\n65                 0       JubalHarshaw23\n66                 0             njman100\n67                 0          No_Yak_6227\n68                 0       Odd_Tiger_2278\n69                 0 platanthera_ciliaris\n70                 0        PoopieButt317\n71                 0  Practical_Shop_4055\n72                 0         RUIN_NATION_\n73                 0       Sea_Engine4333\n74                 0          SeaSuch2077\n75                 0             spotspam\n76                 0          stjoechief1\n77                 0          StormOk7544\n78                 0              syg-123\n79                 0               th1961\n80                 0         Willowgirl78\n\n\nUsers with higher degree centrality are “Knoxcore”, “NoDesinformatziya”, “BrtFrkwr”, “TheRandomInteger”, and “Bulky-You-5657”\nOut of these 5 nodes only one of them (“BrtFrkwr”) is also in the top 5 related to score. So it is consistent with the low correlation between score and centrality.\nLet’s now check what clusters (communities) we do have in this network.\n\n# run clustering algorithm: fast_greedy\npolitik.fg &lt;- igraph::cluster_fast_greedy(politik.ig)\n# inspect clustering object\npolitik.fg\n\nIGRAPH clustering fast greedy, groups: 37, mod: 0.63\n+ groups:\n  $`1`\n   [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n   [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n   [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n  [10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n  [13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n  [16] \"Kamelasa\"           \n  \n  $`2`\n   [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n  + ... omitted several groups/vertices\n\nigraph::groups(politik.fg)\n\n$`1`\n [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n[10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n[13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n[16] \"Kamelasa\"           \n\n$`2`\n [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n [4] \"Roasted_Butt\"        \"BrtFrkwr\"            \"TheRandomInteger\"   \n [7] \"Shaunair\"            \"betterwoke\"          \"DarkwingDuckHunt\"   \n[10] \"Thick-Return1694\"    \"Spara-Extreme\"       \"Admirable_Bad_5649\" \n[13] \"TheReddestOrange\"    \"Scarlettail\"         \"Tower6011\"          \n[16] \"Candid_Chicken_9246\"\n\n$`3`\n[1] \"r-m-russell\"        \"penis_berry_crunch\" \"CecilTWashington\"  \n[4] \"caserock\"           \"gefjunhel\"         \n\n$`4`\n[1] \"iuthnj34\"       \"YeaterdaysQuim\" \"hdiggyh\"        \"Purify5\"       \n\n$`5`\n[1] \"Quiet_Dimensions\" \"Due-Shirt616\"     \"JeffMo\"           \"PineTreeBanjo\"   \n\n$`6`\n[1] \"physical_graffitti\" \"hindusoul\"         \n\n$`7`\n[1] \"LariRed\"         \"InGreedWeTrust3\"\n\n$`8`\n[1] \"Happypappy213\"      \"FijiFanBotNotGay69\"\n\n$`9`\n[1] \"fxkatt\"\n\n$`10`\n[1] \"Sea_Engine4333\"\n\n$`11`\n[1] \"Practical_Shop_4055\"\n\n$`12`\n[1] \"PoopieButt317\"\n\n$`13`\n[1] \"inconsistent3\"\n\n$`14`\n[1] \"Empty-Rise-4409\"\n\n$`15`\n[1] \"stjoechief1\"\n\n$`16`\n[1] \"th1961\"\n\n$`17`\n[1] \"platanthera_ciliaris\"\n\n$`18`\n[1] \"No_Yak_6227\"\n\n$`19`\n[1] \"fore_skin_walker\"\n\n$`20`\n[1] \"HonoredPeople\"\n\n$`21`\n[1] \"bloombergopinion\"\n\n$`22`\n[1] \"RUIN_NATION_\"\n\n$`23`\n[1] \"ImportantNeck491\"\n\n$`24`\n[1] \"grixorbatz\"\n\n$`25`\n[1] \"spotspam\"\n\n$`26`\n[1] \"JubalHarshaw23\"\n\n$`27`\n[1] \"StormOk7544\"\n\n$`28`\n[1] \"njman100\"\n\n$`29`\n[1] \"Odd_Tiger_2278\"\n\n$`30`\n[1] \"cryolongman\"\n\n$`31`\n[1] \"Willowgirl78\"\n\n$`32`\n[1] \"Guttenber\"\n\n$`33`\n[1] \"SeaSuch2077\"\n\n$`34`\n[1] \"syg-123\"\n\n$`35`\n[1] \"bck1999\"\n\n$`36`\n[1] \"Donut131313\"\n\n$`37`\n[1] \"eldred2\"\n\n\nThere are 2 main clusters in the network.\nLet’s see the density of each cluster using block model function.\n\nprint(blockmodel(politik.n, politik.fg$membership)$block.model,\n      digits = 2)\n\n         Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8\nBlock 1     0.62    0.00       0       0       0       0       0       0\nBlock 2     0.00    0.48       0       0       0       0       0       0\nBlock 3     0.00    0.00       1       0       0       0       0       0\nBlock 4     0.00    0.00       0       1       0       0       0       0\nBlock 5     0.00    0.00       0       0       1       0       0       0\nBlock 6     0.00    0.00       0       0       0       1       0       0\nBlock 7     0.00    0.00       0       0       0       0       1       0\nBlock 8     0.00    0.00       0       0       0       0       0       1\nBlock 9     0.00    0.00       0       0       0       0       0       0\nBlock 10    0.00    0.00       0       0       0       0       0       0\nBlock 11    0.00    0.00       0       0       0       0       0       0\nBlock 12    0.00    0.00       0       0       0       0       0       0\nBlock 13    0.00    0.00       0       0       0       0       0       0\nBlock 14    0.00    0.00       0       0       0       0       0       0\nBlock 15    0.00    0.00       0       0       0       0       0       0\nBlock 16    0.00    0.00       0       0       0       0       0       0\nBlock 17    0.00    0.00       0       0       0       0       0       0\nBlock 18    0.00    0.00       0       0       0       0       0       0\nBlock 19    0.00    0.00       0       0       0       0       0       0\nBlock 20    0.00    0.00       0       0       0       0       0       0\nBlock 21    0.00    0.00       0       0       0       0       0       0\nBlock 22    0.00    0.00       0       0       0       0       0       0\nBlock 23    0.00    0.00       0       0       0       0       0       0\nBlock 24    0.00    0.00       0       0       0       0       0       0\nBlock 25    0.00    0.00       0       0       0       0       0       0\nBlock 26    0.00    0.00       0       0       0       0       0       0\nBlock 27    0.00    0.00       0       0       0       0       0       0\nBlock 28    0.00    0.00       0       0       0       0       0       0\nBlock 29    0.00    0.00       0       0       0       0       0       0\nBlock 30    0.00    0.00       0       0       0       0       0       0\nBlock 31    0.00    0.00       0       0       0       0       0       0\nBlock 32    0.00    0.00       0       0       0       0       0       0\nBlock 33    0.00    0.00       0       0       0       0       0       0\nBlock 34    0.00    0.00       0       0       0       0       0       0\nBlock 35    0.00    0.00       0       0       0       0       0       0\nBlock 36    0.00    0.00       0       0       0       0       0       0\nBlock 37    0.00    0.00       0       0       0       0       0       0\n         Block 9 Block 10 Block 11 Block 12 Block 13 Block 14 Block 15 Block 16\nBlock 1        0        0        0        0        0        0        0        0\nBlock 2        0        0        0        0        0        0        0        0\nBlock 3        0        0        0        0        0        0        0        0\nBlock 4        0        0        0        0        0        0        0        0\nBlock 5        0        0        0        0        0        0        0        0\nBlock 6        0        0        0        0        0        0        0        0\nBlock 7        0        0        0        0        0        0        0        0\nBlock 8        0        0        0        0        0        0        0        0\nBlock 9      NaN        0        0        0        0        0        0        0\nBlock 10       0      NaN        0        0        0        0        0        0\nBlock 11       0        0      NaN        0        0        0        0        0\nBlock 12       0        0        0      NaN        0        0        0        0\nBlock 13       0        0        0        0      NaN        0        0        0\nBlock 14       0        0        0        0        0      NaN        0        0\nBlock 15       0        0        0        0        0        0      NaN        0\nBlock 16       0        0        0        0        0        0        0      NaN\nBlock 17       0        0        0        0        0        0        0        0\nBlock 18       0        0        0        0        0        0        0        0\nBlock 19       0        0        0        0        0        0        0        0\nBlock 20       0        0        0        0        0        0        0        0\nBlock 21       0        0        0        0        0        0        0        0\nBlock 22       0        0        0        0        0        0        0        0\nBlock 23       0        0        0        0        0        0        0        0\nBlock 24       0        0        0        0        0        0        0        0\nBlock 25       0        0        0        0        0        0        0        0\nBlock 26       0        0        0        0        0        0        0        0\nBlock 27       0        0        0        0        0        0        0        0\nBlock 28       0        0        0        0        0        0        0        0\nBlock 29       0        0        0        0        0        0        0        0\nBlock 30       0        0        0        0        0        0        0        0\nBlock 31       0        0        0        0        0        0        0        0\nBlock 32       0        0        0        0        0        0        0        0\nBlock 33       0        0        0        0        0        0        0        0\nBlock 34       0        0        0        0        0        0        0        0\nBlock 35       0        0        0        0        0        0        0        0\nBlock 36       0        0        0        0        0        0        0        0\nBlock 37       0        0        0        0        0        0        0        0\n         Block 17 Block 18 Block 19 Block 20 Block 21 Block 22 Block 23\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17      NaN        0        0        0        0        0        0\nBlock 18        0      NaN        0        0        0        0        0\nBlock 19        0        0      NaN        0        0        0        0\nBlock 20        0        0        0      NaN        0        0        0\nBlock 21        0        0        0        0      NaN        0        0\nBlock 22        0        0        0        0        0      NaN        0\nBlock 23        0        0        0        0        0        0      NaN\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 24 Block 25 Block 26 Block 27 Block 28 Block 29 Block 30\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24      NaN        0        0        0        0        0        0\nBlock 25        0      NaN        0        0        0        0        0\nBlock 26        0        0      NaN        0        0        0        0\nBlock 27        0        0        0      NaN        0        0        0\nBlock 28        0        0        0        0      NaN        0        0\nBlock 29        0        0        0        0        0      NaN        0\nBlock 30        0        0        0        0        0        0      NaN\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 31 Block 32 Block 33 Block 34 Block 35 Block 36 Block 37\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31      NaN        0        0        0        0        0        0\nBlock 32        0      NaN        0        0        0        0        0\nBlock 33        0        0      NaN        0        0        0        0\nBlock 34        0        0        0      NaN        0        0        0\nBlock 35        0        0        0        0      NaN        0        0\nBlock 36        0        0        0        0        0      NaN        0\nBlock 37        0        0        0        0        0        0      NaN\n\n\nThe blocks 1 and 2 in our network seems dense.\n\ndf_comm &lt;- data.frame(\n  Node = V(politik.ig)$name,\n  Community = politik.fg$membership,\n  Degree = degree_centrality\n)\n\n# 4. Find maximum degree for each community\n\nhighest_degree_nodes &lt;- df_comm %&gt;%\n  group_by(Community) %&gt;%\n  filter(Degree == max(Degree)) %&gt;%\n  ungroup()\n\nhighest_degree_nodes %&gt;% arrange(-desc(Community))\n\n# A tibble: 51 × 3\n   Node               Community Degree\n   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Knoxcore                   1     15\n 2 NoDesinformatziya          1     15\n 3 BrtFrkwr                   2     14\n 4 r-m-russell                3      4\n 5 penis_berry_crunch         3      4\n 6 CecilTWashington           3      4\n 7 caserock                   3      4\n 8 gefjunhel                  3      4\n 9 iuthnj34                   4      3\n10 YeaterdaysQuim             4      3\n# ℹ 41 more rows\n\n\nLet’s now visualize the communities including the nodes with highest degree centrality for the main communities (1 and 2):\n\n# Identify nodes with high degree centrality (e.g., top 3)\ntop_nodes &lt;- names(sort(degree_centrality, decreasing = TRUE))[1:5]\n\n# Create a custom labeling vector\nnode_labels &lt;- rep(NA, vcount(politik.ig))\n# Set the labels for the nodes with high degree centrality\nnode_labels[top_nodes] &lt;- top_nodes\n\n# Identify isolated nodes\nisolated_nodes &lt;- which(degree(politik.ig) == 0)\n\n# Remove isolated nodes from the network\nnetwork_no_isolated &lt;- delete_vertices(politik.ig, isolated_nodes)\n\nplot(network_no_isolated,\n     vertex.label = node_labels,  \n     vertex.label.cex = 0.8,\n     vertex.color = membership(politik.fg),\n     vertex.label.color = \"black\",\n     vertex.shape = \"sphere\",\n     layout = layout_with_fr,\n     main = \"Communities without isolated nodes and labeling top 5 central nodes\")\n\n\n\n\n\n\n\n\nIt looks like while we have 2 main communities the nodes with highest degree centrality are mostly in one of the communities.\nIt seems like that group of authors are close among them and at the same time are central in the network.\n\n\n\nLastly let’s add a new attribute to the network using text analysis, in particular sentiment analysis.\nLet’s get the sentiment for each author’s comments.\n\nlibrary(sentimentr)\n\n#labeling based on the sentiment score\ncomments &lt;- politik_final3$comment\nget_sentiment_label &lt;- function(ave_sentiment) {\n  if (ave_sentiment &gt; 0.1) {\n    return(\"Positive\")\n  } else if (ave_sentiment &lt; -0.1) {\n    return(\"Negative\")\n  } else {\n    return(\"Neutral\")\n  }\n}\nsentiment_scores &lt;- sentiment_by(x = comments, text.var = comments)\n\n#adding the label for each author in the data set\npolitik_final3$sentiment &lt;- sapply(sentiment_scores$ave_sentiment, get_sentiment_label)\n\nNow let’s visualize the network by sentiment of each node.\n\n# Create a graph object from the data frame\ng &lt;- graph_from_data_frame(politik.n, directed = FALSE)\n\n# Add node attributes to the graph\nV(g)$sentiment &lt;- politik_final3$sentiment[match(V(g)$name, politik_final3$author)]\n\n\n# Define color palette for categories\ncolor_palette &lt;- c(\"Negative\" = \"red\", \"Neutral\" = \"blue\", \"Positive\" = \"lightgreen\")\n\n# Visualize the network with colored nodes\nplot(g, vertex.color = color_palette[V(g)$sentiment], layout = layout_nicely, vertex.label = NA, vertex.size = 7, isolates=TRUE, main = \"Authors Network by Sentiment\")\nlegend(\"bottomright\", legend = c(\"Positive\", \"Neutral\", \"Negative\"), col = c(\"lightgreen\", \"blue\", \"red\"), pch = c(21, 21), title = \"Node Sentiment\")\n\n\n\n\n\n\n\n\nFrom this chart it looks like:\n\nMost of the sentiments are either neutral or negative\nSentiments tend to get closer, or group among them.\n\n\n\n\n\n\n\nThe network was found to be sparse (low density) but highly interconnected within subgroups (high transitivity), suggesting the presence of distinct communities.\nThe visualization of the network confirmed the presence of communities, showing separate clusters of users commenting on Biden and Trump posts. The network was divided into 37 distinct clusters, indicating multiple smaller communities within the larger Biden and Trump-focused groups.\nThe hypothesis that users with higher scores (more upvotes or higher score) would also have more central positions in the network was not supported. The correlation between score and centrality was negligible.\nWhile score wasn’t a strong indicator of influence, degree centrality (number of connections) was used to identify the most connected users. These users may play important roles in shaping discussions within their respective communities.\nIn terms of the sentiment analysis it seems that most comments were neutral or negative. And, users with similar sentiments tended to interact with each other.\n\nThis Network Analysis research offers interesting insights into the social dynamics of a politically charged online community. It highlights the presence of distinct communities, the complex interplay between user engagement (score) and influence (centrality), and the opportunity for further exploration of the factors shaping political discourse on platforms like Reddit."
  }
]