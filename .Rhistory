easy.sum <- 1+1
print(easy.sum)
quarto publish quarto-pub
easy.sum <- 1 + 1
print(easy.sum)
print(easy.div)
easy.div <- 4/2
print(easy.div)
easy.div <- 2/6
print(easy.div)
library(tidytext)
library(dplyr)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(janitor)
#library(RedditExtractoR)
library(RCurl)
library(data.table)
#politics_reddit <- find_thread_urls(subreddit = "politics", sort_by="new", period = "day")
#politics_reddit_comments <- get_thread_content(politics_reddit$url)
#Separate the lists into different objects
#politics_list1 <- politics_reddit_comments[[1]]
#politics_list2 <- politics_reddit_comments[[2]]
### Saving all the lists in csv to avoid the need to scrap the data several times
#write.csv(politics_reddit, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv")
#write.csv(politics_list1, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv")
#write.csv(politics_list2, file = "C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv")
getwd()
# Read large CSV file using fread
politik1 <- fread("politics_comments1.csv")
politik2 <- fread("politics_comments2.csv")
politik3 <- fread("politics_comments3.csv")
glimpse(politik1)
glimpse(politik2)
glimpse(politik3)
# Cleaning and wrangling
politik_df <- politik2 %>% select(-V1, -timestamp) #eliminating non-relevant columns
politik_df <- as_tibble(politik_df)
politik_df$date <- as.Date(politik_df$date, format = "%m/%d/%Y")
politik_df2 <- politik3 %>% select(-V1, -timestamp) #eliminating non-relevant columns
politik_df2 <- as_tibble(politik_df2)
politik_df2$date <- as.Date(politik_df2$date, format = "%m/%d/%Y")
politik_df2 <- politik_df2[-(which(politik_df2$author %in% "AutoModerator")),]
politik_df3 <- politik_df2[-(which(politik_df2$author %in% "[deleted]")),]
length(unique(politik3$url))
#let's create some tables to see frequencies and totals
#first I created a count column
politik_df3 <- politik_df3 %>% mutate(countid = "1")
politik_df3$countid <- as.numeric(politik_df3$countid)
#preparing tables
library(data.table)
politik_table2 <- data.table(politik_df3)
#total posts grouped by author
count_table2 <- politik_table2 %>% group_by(author) %>% summarise(Total_posts = sum(countid))
count_table2 <- count_table2 %>% arrange(desc(Total_posts))
print(count_table2)
summary_votes <- politik_table2 %>% group_by(author) %>% summarize(Total_Score = sum(score))
summary_votes <- summary_votes %>% arrange(desc(Total_Score))
print(summary_votes)
#Upvotes as a proportion of comments
summary_votes_ratio <- politik_table2 %>% group_by(author) %>% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))
summary_votes_ratio <- summary_votes_ratio %>% arrange(desc(Ratio_upvotes_per_comment))
print(summary_votes_ratio)
#How many authors (nodes) we have here?
length(unique(politik_df3$author))
#first let's see the distribution of number of comments
percentiles <- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles)
subset_politik2 <- subset(politik_df, comments >= 1439 )
glimpse(subset_politik2)
length(unique(subset_politik2$author))
subset_politik3 <- politik_df3 %>%
filter(url %in% subset_politik2$url)
#let's see the df now
glimpse(subset_politik3)
#how many nodes (authors)?
length(unique(subset_politik3$author))
#First selecting posts with "Trump" or "Biden" included in the title of the post
#Filtering the titles that contain Trump
trump_df <- politik_df %>% filter(grepl("Trump", title))
trump_df$candidate <- "Trump"
#Let's check the distribution of number of comments
percentiles_trump <- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles_trump)
#Filtering the titles that contain Biden
biden_df <- politik_df %>% filter(grepl("Biden", title))
biden_df$candidate <- "Biden"
#Let's check the distribution of number of comments
percentiles_biden <- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))
print(percentiles_biden)
#Selecting the post for Trump and Biden that we will analyze
#Let's choose one post for each presidential candidate
#based on the median number of comments for each
#Trump
trump_post <- subset(trump_df, comments == 74 )
#Biden
biden_post <- subset(biden_df, comments == 67 )
#merging the previous df's
trump_biden_df <- rbind(trump_post, biden_post)
print(trump_biden_df$url)
#let's identify these posts in the politik3 df (containing all the details)
subset_politik3 <- politik_df3 %>%
filter(url %in% trump_biden_df$url)
#creating a new column with the candidate related to the post
subset_politik3 <- subset_politik3 %>%
mutate(candidate = case_when(
url == "https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/" ~ "Trump",
url == "https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/" ~ "Biden",
))
#let's see the df now
glimpse(subset_politik3)
# Let's keep only the relevant columns
politik_final <- select(subset_politik3, c("url", "author", "score", "comment", "comment_id", "candidate"))
# Extracting the levels of each comment and its hierarchy
politik_final2 <- politik_final %>%
mutate(Level = str_count(comment_id, pattern = "_") + 1,  # Count underscores to determine depth
ParentID = ifelse(Level > 1, sapply(strsplit(comment_id, "_"), function(x) paste(x[-length(x)], collapse = "_")), NA))
length(unique(politik_final2$author))
length(unique(politik_final2$url))
#Rename level column as it represent more how deep/far is the comment
#from the initial post, we will use this later as an attribute
politik_final2 <- politik_final2 %>%
rename(deep = Level)
#identify who is commenting on the same post
politik_final2 <- politik_final2 %>%
mutate(level = substr(comment_id, 1, 2))
politik_final2$level <- str_replace_all(politik_final2$level, "_", "")
politik_final2 <- politik_final2 %>%
mutate(level2 = substr(candidate, 1, 1))
politik_final2$comment_id2 <- paste(politik_final2$level2, politik_final2$level, sep = "_")
#Now I'll create a new object by keeping only the columns I need
politik_final3 <- select(politik_final2, c(-"comment_id", -"ParentID", -"level", -"level2"))
#Will create a attribute only object to use later
politik_attributes <- select(politik_final3, c("score", "candidate", "deep"))
head(politik_attributes)
politik_m <- select(politik_final3, c("comment_id2", "author"))
glimpse(politik_m)
# Identify unique names and codes
unique_names <- unique(politik_final3$author)
unique_codes <- unique(politik_final3$comment_id2)
# Create an empty adjacency matrix
adj_matrix <- matrix(0, nrow = length(unique_names), ncol = length(unique_names),
dimnames = list(unique_names, unique_names))
#Populate the adjacency matrix based on shared codes
for (i in 1:length(unique_names)) {
for (j in 1:length(unique_names)) {
# Check if names i and j have the same code
shared_code <- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],
politik_final3$comment_id2[politik_final3$author == unique_names[j]])
if (length(shared_code) > 0) {
adj_matrix[unique_names[i], unique_names[j]] <- 1  # Set relationship to 1
}
}
}
# I'll eliminate loops in advance
diag(adj_matrix) <- 0
#load packages
library(network)
library(sna)
library(statnet)
politik.n <- network(adj_matrix)
politik.n
#Dyads and Triads census
sna::dyad.census(politik.n)
sna::triad.census(politik.n)
sum(sna::triad.census(politik.n))
#transitivity
gtrans(politik.n, mode="graph")
# get network density: statnet
network::network.density(politik.n) #already exclude loops
# Plot the network
plot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = "Authors Network")
# Plot the network
plot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = "Authors Network")
#First I'll create a column with female-male true-false attribute
politik_attributes2 <- politik_attributes %>%
mutate(
biden = if_else(candidate == "Biden", "TRUE", "FALSE")
)
#now let's see how females and males are interacting
nodeColors<-ifelse(politik_attributes2$biden,"dodgerblue","red")
plot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=T) #including isolated nodes
legend("bottomright", legend = c("Biden", "Trump"), col = c("dodgerblue", "red"), pch = c(21, 21), title = "Node Type")
plot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=F) #excluding isolated nodes
legend("bottomright", legend = c("Biden", "Trump"), col = c("dodgerblue", "red"), pch = c(21, 21), title = "Node Type")
#preparing for text analysis
politik_vec <- as.vector(politik_final2$comment)
politik_vec2 <- unlist(politik_vec, use.names = FALSE)
politik_corpus <- corpus(politik_vec2)
politik_corpus_summary <- summary(politik_corpus)
politik_tokens <- tokens(politik_corpus)
politik_tokens2 <- tokens(politik_corpus, remove_punct=TRUE, remove_numbers = T) %>%
tokens_select(pattern=c(stopwords("en"), "s", "just", "get", "can", "like", "people"),
selection="remove") %>%
dfm()
#word cloud
textplot_wordcloud(politik_tokens2, max.words = 200)
version
