{
  "hash": "9bd001fbe64cbc95f6b599f656b26084",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Final Project FB - working doc\"\nsubtitle: \"DACSS 695N Social Network Analysis\"\nauthor: \"E. Song/ Felix Betancourt\"\ndate: \"April 21, 2024\"\nformat: \n  html:\n    toc: true\n    toc-depth: 2\n    toc-title: Contents\n    toc-location: left\n    code-fold: false\n    html-math-method: katex\n    theme: flatly\n    smooth-scroll: true\n    link-external-icon: true\n    link-external-newwindow: true\n    citations-hover: true\n    footnotes-hover: true\n    font-size: 80%\neditor: visual\n---\n\n\n## Final Project - Network on subreddit r/politics\n\n### Introduction\n\nI am interested in understanding how social media users influence each other and create communities around specific topics.\n\nSpecifically, I would like to explore this topic using Reddit to understand more in the context of domestic Politics.\n\n### Research Question\n\nIn particular:\n\n1.  How are Reddit users connected/related in the \"Politics\" subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\n\n2.  Are there different communities (networks) for Biden and Trump?\n\n3.  Is there a relationship between \"upvotes\" for a post, number of comments and how it is related to the key users in the network?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\n#library(RedditExtractoR)\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n```\n:::\n\n\n### Data\n\n1.  I scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package)\n\nThe code below is inactive because it can take a lot of time to run it. So I already scrapped the data and saved the files as csv.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#politics_reddit <- find_thread_urls(subreddit = \"politics\", sort_by=\"new\", period = \"day\")\n\n#politics_reddit_comments <- get_thread_content(politics_reddit$url)\n\n#Separate the lists into different objects\n\n#politics_list1 <- politics_reddit_comments[[1]] \n#politics_list2 <- politics_reddit_comments[[2]]\n\n### Saving all the lists in csv to avoid the need to scrap the data several times\n\n#write.csv(politics_reddit, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments1.csv\")\n\n#write.csv(politics_list1, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments2.csv\")\n\n#write.csv(politics_list2, file = \"C:/Users/fbeta/OneDrive/1_UMASS_DACSS/695N - Social Network/R-695N-Network/Final Project/data/politics_comments3.csv\")\n```\n:::\n\n\n2.  This subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C:/Users/FelixBetancourt/OneDrive - H-E Parts International/Personal/DACSS-HEP/695N- Network/695NBlog_Felix_Betancourt/posts/Post 3 - Initial Analysis\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Read large CSV file using fread\npolitik1 <- fread(\"politics_comments1.csv\")\npolitik2 <- fread(\"politics_comments2.csv\")\npolitik3 <- fread(\"politics_comments3.csv\")\n\nglimpse(politik1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 983\nColumns: 8\n$ V1        <chr> \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  <IDate> 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp <int> 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     <chr> \"Supreme Court starts arguments as Biden administration defe…\n$ text      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit <chr> \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  <int> 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       <chr> \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(politik2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 983\nColumns: 16\n$ V1                    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   <chr> \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                <chr> \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  <chr> \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             <int> 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 <chr> \"Supreme Court starts arguments as Biden adminis…\n$ text                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             <chr> \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 <int> 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               <int> 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              <dbl> 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           <int> 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              <int> 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 95,879\nColumns: 11\n$ V1         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     <chr> \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       <chr> \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  <int> 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      <int> 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    <int> 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id <chr> \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n```\n\n\n:::\n:::\n\n\nAs we can see the the information in object \"politik1\" is redundant with the information in \"politik2\" so I won't use \"politik1\" at all. \"Politik2\" contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. \"politik3\" contain detailed comments on each post and the hierarchical sequence of comments to each post.\n\nI'll use Text as Data methods to identify key words in the title of the posts (like Biden-Trump, or other topics of interest).\n\nLet's do some data wrangling first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cleaning and wrangling\n\npolitik_df <- politik2 %>% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df <- as_tibble(politik_df)\npolitik_df$date <- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 <- politik3 %>% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 <- as_tibble(politik_df2)\npolitik_df2$date <- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n```\n:::\n\n\nLooking at the comments from user \"Automoderator\", it is like a Reddit moderator bot reminding rules of the forum, so I'll delete the rows belonging to AutoModerator\". Also there are few commments where the author was \"deleted\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitik_df2 <- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 <- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\nlength(unique(politik3$url))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 983\n```\n\n\n:::\n:::\n\n\nLet's explore a bit about the authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_df3 <- politik_df3 %>% mutate(countid = \"1\")\npolitik_df3$countid <- as.numeric(politik_df3$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 <- data.table(politik_df3)\n\n#total posts grouped by author\ncount_table2 <- politik_table2 %>% group_by(author) %>% summarise(Total_posts = sum(countid))\ncount_table2 <- count_table2 %>% arrange(desc(Total_posts))\nprint(count_table2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31,554 × 2\n   author               Total_posts\n   <chr>                      <dbl>\n 1 Numerous_Photograph9         304\n 2 Logical_Parameters           223\n 3 JubalHarshaw23               188\n 4 BrtFrkwr                     178\n 5 Due-Shirt616                 148\n 6 TintedApostle                148\n 7 Alistazia                    142\n 8 LibertyInaFeatherBed         121\n 9 grixorbatz                   118\n10 RickyWinterborn-1080          97\n# ℹ 31,544 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary_votes <- politik_table2 %>% group_by(author) %>% summarize(Total_Score = sum(score))\nsummary_votes <- summary_votes %>% arrange(desc(Total_Score))\nprint(summary_votes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31,554 × 2\n   author             Total_Score\n   <chr>                    <int>\n 1 Jackinapox               15912\n 2 jackleggjr               12829\n 3 tracch                   12462\n 4 OsellusK                 11978\n 5 BlotchComics             11743\n 6 BukkitCrab               11161\n 7 OppositeDifference       10184\n 8 zsreport                  8899\n 9 JubalHarshaw23            8694\n10 AngusMcTibbins            8477\n# ℹ 31,544 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n#Upvotes as a proportion of comments\nsummary_votes_ratio <- politik_table2 %>% group_by(author) %>% summarize(Ratio_upvotes_per_comment = sum(upvotes)/sum(countid))\nsummary_votes_ratio <- summary_votes_ratio %>% arrange(desc(Ratio_upvotes_per_comment))\nprint(summary_votes_ratio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31,554 × 2\n   author             Ratio_upvotes_per_comment\n   <chr>                                  <dbl>\n 1 tracch                                 6231 \n 2 IMissChannel76                         4573 \n 3 TheDudeBeto                            3892 \n 4 Mikraphonechekka12                     3728 \n 5 OokLeeNooma                            2746.\n 6 giddyviewer                            2341 \n 7 pottymcnugg                            2340 \n 8 Fairymask                              2277 \n 9 Manikin_Maker                          2177 \n10 torspice                               2064 \n# ℹ 31,544 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 31554\n```\n\n\n:::\n:::\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I'll select a sample of posts to analyze.\n\nI'll select the top 1% posts with more comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#first let's see the distribution of number of comments\npercentiles <- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n```\n\n\n:::\n:::\n\n\nLet's subset the df with the top 1% posts in terms of comments and let's see how many posts we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubset_politik2 <- subset(politik_df, comments >= 1439 )\nglimpse(subset_politik2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 14\n$ url                   <chr> \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                <chr> \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  <date> 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 <chr> \"Letitia James fires back after Donald Trump's b…\n$ text                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             <chr> \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 <int> 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               <int> 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              <dbl> 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           <int> 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              <int> 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(subset_politik2$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\nWe got a df with 10 original posts and 10 authors, this is now a more \"reasonable\" data frame to analyze.\n\nNow I need to identify these post into the \"politik_df3\" df which contain all the hierarchical comments network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubset_politik3 <- politik_df3 %>%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,902\nColumns: 10\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     <chr> \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       <date> 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      <int> 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    <int> 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id <chr> \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3869\n```\n\n\n:::\n:::\n\n\nWe got 982 posts but still +3.8k nodes, it sounds still high number of nodes.\n\nI'll need a different approach.\n\nI'll select 2 posts with a \"median\" number of comments. One post will be about Trump and another about Biden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n#Filtering the titles that contain Trump\ntrump_df <- politik_df %>% filter(grepl(\"Trump\", title))\ntrump_df$candidate <- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump <- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n#Filtering the titles that contain Biden\nbiden_df <- politik_df %>% filter(grepl(\"Biden\", title))\nbiden_df$candidate <- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden <- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Selecting the post for Trump and Biden that we will analyze\n#Let's choose one post for each presidential candidate\n#based on the median number of comments for each\n\n#Trump\ntrump_post <- subset(trump_df, comments == 74 )\n\n#Biden\nbiden_post <- subset(biden_df, comments == 67 )\n```\n:::\n\n\nNow I got the 2 main posts, let's explore a bit those 2 posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#merging the previous df's\ntrump_biden_df <- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n```\n\n\n:::\n\n```{.r .cell-code}\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 <- politik_df3 %>%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 <- subset_politik3 %>%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 119\nColumns: 11\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     <chr> \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       <date> 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      <int> 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    <int> 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"It's very possible that both Biden and Trump are losing so…\n$ comment_id <chr> \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  <chr> \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Let's keep only the relevant columns\npolitik_final <- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 <- politik_final %>%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level > 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 80\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(politik_final2$url))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\nNow I am ready to work on this data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 <- politik_final2 %>%\n  rename(deep = Level)\n\n#identify who is commenting on the same post\npolitik_final2 <- politik_final2 %>%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level <- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 <- politik_final2 %>%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 <- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 <- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes <- select(politik_final3, c(\"score\", \"candidate\", \"deep\"))\nhead(politik_attributes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  score candidate  deep\n  <int> <chr>     <dbl>\n1    20 Biden         1\n2    27 Biden         1\n3     3 Biden         2\n4   -18 Biden         2\n5    23 Biden         3\n6   -12 Biden         4\n```\n\n\n:::\n:::\n\n\nI'll prepare the adjacency matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitik_m <- select(politik_final3, c(\"comment_id2\", \"author\"))\nglimpse(politik_m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 119\nColumns: 2\n$ comment_id2 <chr> \"B_2\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B_3\", \"B…\n$ author      <chr> \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"Angus…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Identify unique names and codes\nunique_names <- unique(politik_final3$author)\nunique_codes <- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix <- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code <- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) > 0) {\n      adj_matrix[unique_names[i], unique_names[j]] <- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) <- 0\n```\n:::\n\n\nNow let's explore the Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\n#load packages\nlibrary(network)\nlibrary(sna)\nlibrary(statnet)\n})\npolitik.n <- network(adj_matrix)\npolitik.n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Network attributes:\n  vertices = 80 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 316 \n    missing edges= 0 \n    non-missing edges= 316 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n```\n\n\n:::\n:::\n\n\nWe got 80 nodes and 316 edges.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Mut Asym Null\n[1,] 158    0 3002\n```\n\n\n:::\n\n```{.r .cell-code}\nsna::triad.census(politik.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(sna::triad.census(politik.n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 82160\n```\n\n\n:::\n:::\n\n\nSeems that we have 82160 triads in total!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8495798\n```\n\n\n:::\n:::\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\n\nLet's see the density\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05\n```\n\n\n:::\n:::\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\n\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\n\nLet's visualize the network\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nWithout isolated nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nLet's now include the Candidate as attribute\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#First I'll create a column with female-male true-false attribute\npolitik_attributes2 <- politik_attributes %>%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how females and males are interacting\nnodeColors<-ifelse(politik_attributes2$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=T) #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=2, displayisolates=F) #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}